{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B2RhJ3GVxwGR"
      },
      "outputs": [],
      "source": [
        "%pip install torch==1.11.0+cu113 torchdata==0.3.0 torchtext==0.12.0 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "%pip install spacy tqdm\n",
        "%pip install datasets\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "T8bm5matxdEF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "szjQ0dRjxdEF"
      },
      "outputs": [],
      "source": [
        "train = dataset[\"train\"]\n",
        "valid = dataset[\"validation\"]\n",
        "test = dataset[\"test\"]\n",
        "\n",
        "\n",
        "train_tokens = train[\"tokens\"]\n",
        "train_labels = train[\"ner_tags\"]\n",
        "valid_tokens = valid[\"tokens\"]\n",
        "valid_labels = valid[\"ner_tags\"]\n",
        "test_tokens = test[\"tokens\"]\n",
        "test_labels = test[\"ner_tags\"]\t\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        }
      ],
      "source": [
        "#find maximum sequence length\n",
        "max_length = 0\n",
        "for i in train_tokens:\n",
        "  print(len(i))\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.']\n",
            "['The', 'following', 'physiological', 'traits', 'were', 'measured', ':', 'stomatal', 'conductance', '(', 'gs', ',', 'mol', 'H2O', 'm-2', 's-1', ')', ',', 'transpiration', 'rate', '(', 'E', ',', 'mmol', 'H2O', 'm-2', 's-1', ')', ',', 'net', 'photosynthetic', 'rate', '(', 'PN', ',', 'μmol', 'm-2', 's-1', ')', 'and', 'intercellular', 'CO2', 'concentration', 'CO2', '(', 'Ci', ',', 'μmol', 'm-2', 's-1', ')', '.']\n",
            "['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O']\n",
            "['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-AC', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O']\n"
          ]
        }
      ],
      "source": [
        "print(train_tokens[0])\n",
        "print(train_tokens[1])\n",
        "print(train_labels[0])\n",
        "print(train_labels[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "#convert the token lists to sentences\n",
        "def tokens_to_sentences(tokens):\n",
        "\tsentences = []\n",
        "\tfor token in tokens:\n",
        "\t\tsentences.append(\" \".join(token))\n",
        "\treturn sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_sentences = tokens_to_sentences(train_tokens)\n",
        "valid_sentences = tokens_to_sentences(valid_tokens)\n",
        "test_sentences = tokens_to_sentences(test_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "323\n",
            "( EGF , epidermal growth factor ; TGF , transforming growth factor ; BTC , betacellulin ; HB - EGF , heparin - binding epidermal growth factor ( EGF)-like growth factor ; EREG , epiregulin ; NRG1 , neuregulin-1 ; NRG2 , neuregulin-2 ; NRG3 , neuregulin-3 ; NRG4 , neuregulin-4 ; PLCγ , phospholipase C type gamma ; CAMK2B , calcium / calmodulin dependent protein kinase ; PRKCB , Protein kinase C - beta ; STAT5 , Signal transducer and activator of transcription 5 ; src , Rous sarcoma virus gene ; CRK , C T10 regulator of a tyrosine kinase ; NCL , NCK Adaptor Protein 2 ; PTK2 , PTK2 protein tyrosine kinase 2 ; ABL2 , V - Abl Abelson Murine Leukemia Viral Oncogene Homolog 2 ; PAK2 , P21 ( RAC1 ) Activated Kinase 2 ; MAP2K4 , Mitogen - Activated Protein Kinase Kinase 4 ; MAPK10 , Mitogen - Activated Protein Kinase 10 ; SOS1 , SOS Ras / Rac Guanine Nucleotide Exchange Factor 1 ; Grb2 , Growth Factor Receptor Bound Protein 2 ; SHC4 , Src Homology 2 Domain - Containing - Transforming Protein C4 ; PIK3C4 , Phosphatidylinositol-4,5 - Bisphosphate 3 - Kinase Catalytic Subunit ; AKT3 , KT Serine / Threonine Kinase 3 ; mTOR , Mechanistic Target Of Rapamycin Kinase ; BCL2 , BCL2 Associated Agonist Of Cell Death ; GSK3B , Glycogen Synthase Kinase 3 Beta ; CDKN1A , Cyclin Dependent Kinase Inhibitor 1A ; EIF4EBP1 , Eukaryotic Translation Initiation Factor 4E Binding Protein 1 ; BRAF , B - Raf Proto - Oncogene , Serine / Threonine Kinase ; RPS6KB1 , Ribosomal Protein S6 Kinase B1 ; KRAS , KRAS Proto - Oncogene , GTPase ; JUN , Jun Proto - Oncogene , AP-1 Transcription Factor Subunit ; ELK , ETS Transcription Factor ; Myc , MYC Proto - Oncogene , BHLH Transcription Factor ; ER , endoplasmic reticulum .\n"
          ]
        }
      ],
      "source": [
        "#find longest sentence\n",
        "max_length = 0\n",
        "longest = None\n",
        "for i,v in enumerate(train_tokens):\n",
        "\tif len(v) > max_length:\n",
        "\t\tmax_length = len(v)\t\n",
        "\t\tlongest =v\n",
        "print(max_length)\n",
        "print(\" \".join(longest))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.']\n",
            "['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O']\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "sentences = train_tokens\n",
        "labels = train_labels\n",
        "print(sentences[0])\n",
        "print(labels[0])\n",
        "token_tokenizer = Tokenizer(lower=True, oov_token=\"<UNK>\")\n",
        "token_tokenizer.fit_on_texts(sentences)\n",
        "word_index = token_tokenizer.word_index\n",
        "word_index[\"PAD\"] = 0  # Ensure padding token is 0\n",
        "sequences = token_tokenizer.texts_to_sequences(sentences)\n",
        "max_seq_length = max(len(s) for s in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post', value=word_index[\"PAD\"])\n",
        "\n",
        "label_tokenizer = Tokenizer(lower=False)\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "label_index = label_tokenizer.word_index\n",
        "\n",
        "print(len(label_index))\n",
        "num_labels = len(label_index) + 1 \n",
        "label_sequences = label_tokenizer.texts_to_sequences(labels)\n",
        "padded_label_sequences = pad_sequences(label_sequences, maxlen=max_seq_length, padding='post', value=0)\n",
        "\n",
        "label_mask = (padded_label_sequences != 0).astype(np.float32)\n",
        "\n",
        "num_classes = len(label_index) + 1  # Assuming 0 is not used by label_index\n",
        "one_hot_labels = np.zeros((padded_label_sequences.shape[0], padded_label_sequences.shape[1], num_classes))\n",
        "\n",
        "for i, sequence in enumerate(padded_label_sequences):\n",
        "    for j, label in enumerate(sequence):\n",
        "        if label != 0:  # Only create one-hot vectors for non-padding labels\n",
        "            one_hot_labels[i, j, label] = 1\n",
        "\n",
        "one_hot_labels = np.array(one_hot_labels)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n",
            "323\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "example = label_sequences[0]\n",
        "print(len(example))\n",
        "example = one_hot_labels[0]\n",
        "\n",
        "print(len(example))\n",
        "print(example[15:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\liamd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 102ms/step - accuracy: 0.2006 - loss: 1.6036\n",
            "Epoch 2/10\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.1072 - loss: 1.5356\n",
            "Epoch 3/10\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.0786 - loss: 1.3981\n",
            "Epoch 4/10\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.0836 - loss: 1.2602\n",
            "Epoch 5/10\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.0892 - loss: 1.1395\n",
            "Epoch 6/10\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.0910 - loss: 1.0505\n",
            "Epoch 7/10\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.0937 - loss: 0.9734\n",
            "Epoch 8/10\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - accuracy: 0.0926 - loss: 0.9316\n",
            "Epoch 9/10\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.0923 - loss: 0.8889\n",
            "Epoch 10/10\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.0954 - loss: 0.8484\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x1d27b41f860>"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define an RNN in TensorFlow/Keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, TimeDistributed, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index), output_dim=64, input_length=max_seq_length, mask_zero=True))\n",
        "model.add(SimpleRNN(units=64, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(num_labels, activation='softmax')))\n",
        "\n",
        "# Compile the model\n",
        "adam = Adam(learning_rate=2e-5)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_sequences, one_hot_labels, batch_size=32, epochs=10, sample_weight = label_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "[1 2 1 4 2 1 1 2 1 2 1 1 1 1 1 1 1 2 1 0 4 1 1 2 1 1 4 1 1 1 2 4 1 1 1 1 2\n",
            " 1 1 3 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-AC', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O']\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import argmax\n",
        "\n",
        "# Let's assume `new_sentences` is a list of new text data you want to predict.\n",
        "new_sentences = train_sentences[:2]\n",
        "new_labels = train_labels[:2]\n",
        "\n",
        "# Tokenize the new sentences using the same tokenizer instance\n",
        "new_sequences = token_tokenizer.texts_to_sequences(new_sentences)\n",
        "\n",
        "# Pad the new sequences\n",
        "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_seq_length, padding='post', value=word_index[\"PAD\"])\n",
        "\n",
        "# Predict classes with the model\n",
        "predictions = model.predict(new_padded_sequences)\n",
        "\n",
        "# Taking argmax to get the most likely class index\n",
        "predicted_class_indices = argmax(predictions, axis=-1)\n",
        "print(predicted_class_indices[1])\n",
        "print(new_labels[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'PAD'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[150], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Pad the sequences to have the same length as the training data.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m padded_test_sequences \u001b[38;5;241m=\u001b[39m pad_sequences(test_sequences, maxlen\u001b[38;5;241m=\u001b[39mmax_seq_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39mword_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPAD\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m padded_test_label_sequences \u001b[38;5;241m=\u001b[39m pad_sequences(test_label_sequences, maxlen\u001b[38;5;241m=\u001b[39mmax_seq_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[43mlabel_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     12\u001b[0m one_hot_test_labels \u001b[38;5;241m=\u001b[39m [[to_categorical(i, num_classes\u001b[38;5;241m=\u001b[39mnum_labels) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m padded_test_label_sequences]\n\u001b[0;32m     13\u001b[0m one_hot_test_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(one_hot_test_labels)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'PAD'"
          ]
        }
      ],
      "source": [
        "# test the test set against the model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "test_sequences = token_tokenizer.texts_to_sequences(test_tokens)\n",
        "test_label_sequences = label_tokenizer.texts_to_sequences(test_labels)\n",
        "\n",
        "# Pad the sequences to have the same length as the training data.\n",
        "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_seq_length, padding='post', value=word_index[\"PAD\"])\n",
        "\n",
        "padded_test_label_sequences = pad_sequences(test_label_sequences, maxlen=max_seq_length, padding='post', value=label_index[\"PAD\"])\n",
        "one_hot_test_labels = [[to_categorical(i, num_classes=num_labels) for i in label] for label in padded_test_label_sequences]\n",
        "one_hot_test_labels = np.array(one_hot_test_labels)\n",
        "\n",
        "# Predict on the test data.\n",
        "loss, accuracy  = model.evaluate(padded_test_sequences, one_hot_test_labels)\n",
        "loss, accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
