{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nkyPsvNE7PO"
      },
      "source": [
        "# Data Pre-Processing techniques\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9cisibXE7PP"
      },
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkNLNhODE7PP"
      },
      "outputs": [],
      "source": [
        "%pip install datasets\n",
        "%pip install transformers\n",
        "%pip install spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M7i41ePE7PP"
      },
      "source": [
        "BELOW TAKES FOREVER!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au0thEZNE7PP"
      },
      "outputs": [],
      "source": [
        "%pip install spacy-transformers\n",
        "%pip install transformers[torch]\n",
        "%pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBaVZWOlE7PQ",
        "outputId": "45b2a646-0ea0-44f9-98a7-7d47d23e4084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  2.2.1+cu121\n",
            "torchtext Version:  0.17.1+cpu\n",
            "Using GPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "\n",
        "SEED = 1234\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"torchtext Version: \", torchtext.__version__)\n",
        "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RmQTujIrE7PQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwEgDET0E7PQ"
      },
      "source": [
        "## Load BERT Model from transformers library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3F5mQp-E7PQ",
        "outputId": "d409314c-328a-4a73-b366-2f0db8b2b69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twOggBbtE7PQ"
      },
      "outputs": [],
      "source": [
        "training = dataset[\"train\"]\n",
        "valid = dataset[\"validation\"]\n",
        "test = dataset[\"test\"]\n",
        "\n",
        "training_tokens = training[\"tokens\"]\n",
        "training_labels = training[\"ner_tags\"]\n",
        "valid_tokens = valid[\"tokens\"]\n",
        "valid_labels = valid[\"ner_tags\"]\n",
        "test_tokens = test[\"tokens\"]\n",
        "test_labels = test[\"ner_tags\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3ZXei4KE7PR"
      },
      "source": [
        "Function for forming ngrams out of tokens and labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8wYv770nE7PR"
      },
      "outputs": [],
      "source": [
        "def n_grams(tokens, labels, n):\n",
        "  all_ngrams = []\n",
        "  all_labels = []\n",
        "  for i in range(0,len(tokens)):\n",
        "    for j in range(len(tokens[i]) - n +1):\n",
        "      all_ngrams.append(tokens[i][j:j+n])\n",
        "      all_labels.append(labels[i][j:j+n])\n",
        "  return all_ngrams, all_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzFbClZ7E7PR"
      },
      "source": [
        "Form n-grams for the different dataset splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "0SkTHzz0E7PR",
        "outputId": "209e2c7b-3ed4-41be-d381-7815b28306c5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'training_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-e77eed1f5013>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_grams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalid_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_grams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_grams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_tokens' is not defined"
          ]
        }
      ],
      "source": [
        "training_ngrams, training_ner = n_grams(training_tokens, training_labels, 2)\n",
        "valid_ngrams, valid_ner = n_grams(valid_tokens, valid_labels, 2)\n",
        "test_ngrams, test_ner = n_grams(test_tokens, test_labels, 2)\n",
        "len(training_ngrams), len(valid_ngrams), len(test_ngrams)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6FPSqPJE7PR"
      },
      "source": [
        "# Need to convert labels to class indexes so that the model can understand them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hiiOzZq1E7PR"
      },
      "outputs": [],
      "source": [
        "def conv_label_indexes(training, valid, test ):\n",
        "\tlabel_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}\n",
        "\n",
        "\tlabel_list = []\n",
        "\tfor sample in training:\n",
        "\t\tlabel_list.append([label_encoding[tag] for tag in sample])\n",
        "\n",
        "\tval_label_list = []\n",
        "\tfor sample in valid:\n",
        "\t\tval_label_list.append([label_encoding[tag] for tag in sample])\n",
        "\n",
        "\ttest_label_list = []\n",
        "\tfor sample in test:\n",
        "\t\ttest_label_list.append([label_encoding[tag] for tag in sample])\n",
        "\treturn label_list, val_label_list, test_label_list\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_list, val_label_list, test_label_list = conv_label_indexes(training_ner, valid_ner, test_ner)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "k76eYjk4rwu6",
        "outputId": "136c2115-6513-448e-89e3-65a3b5642083"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'training_ner' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-865a9b9e3210>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_label_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_ner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'training_ner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "B0P0y9g0E7PR"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(ngrams, list_name):\n",
        "    tokenized_inputs = tokenizer(ngrams, truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(list_name):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cGci6JfKE7PR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "a7e48236-a6e6-46c1-d89d-28235b833e54"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'training_ngrams' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-21fe2f16b7aa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_align_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenized_val_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_align_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenized_test_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_align_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(tokenized_datasets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_ngrams' is not defined"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = tokenize_and_align_labels(training_ngrams, label_list)\n",
        "tokenized_val_datasets = tokenize_and_align_labels(valid_ngrams, val_label_list)\n",
        "tokenized_test_datasets = tokenize_and_align_labels(test_ngrams, test_label_list)\n",
        "# print(tokenized_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "U_2RyufcE7PR"
      },
      "outputs": [],
      "source": [
        "# BERT's tokenizer returns the dataset in the form of a dictionary of lists (sentences).\n",
        "# we have to convert it into a list of dictionaries for training.\n",
        "def turn_dict_to_list_of_dict(d):\n",
        "    new_list = []\n",
        "\n",
        "    for labels, inputs in zip(d[\"labels\"], d[\"input_ids\"]):\n",
        "        entry = {\"input_ids\": inputs, \"labels\": labels}\n",
        "        new_list.append(entry)\n",
        "\n",
        "    return new_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hlnvjkk0E7PS"
      },
      "outputs": [],
      "source": [
        "tokenised_train = turn_dict_to_list_of_dict(tokenized_datasets)\n",
        "tokenised_val = turn_dict_to_list_of_dict(tokenized_val_datasets)\n",
        "tokenised_test = turn_dict_to_list_of_dict(tokenized_test_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "MRt62HQYE7PS"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRvReV1_E7PS",
        "outputId": "b2602c85-4c4f-491f-8ad6-582ae508f1dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "metric = load_metric(\"seqeval\")\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYBYtQ_-E7PS",
        "outputId": "b8b6eb8b-da11-4175-8cb1-da32de6cf4a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "\n",
        "# Training arguments (feel free to play arround with these values)\n",
        "model_name = \"bert-base-uncased\"\n",
        "epochs = 6\n",
        "batch_size = 4\n",
        "learning_rate = 2e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"BERT-finetuned-NER\",\n",
        "    # evaluation_strategy = \"epoch\", ## Instead of focusing on loss and accuracy, we will focus on the F1 score\n",
        "    evaluation_strategy ='steps',\n",
        "    eval_steps = 7000,\n",
        "    save_total_limit = 3,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.001,\n",
        "    save_steps=35000,\n",
        "    metric_for_best_model = 'f1',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenised_train,\n",
        "    eval_dataset=tokenised_val,\n",
        "    data_collator = data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "id": "9dz42HZzE7PS",
        "outputId": "aca78d9a-f7d5-4866-ca3c-0b5982d59f1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='58392' max='58392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [58392/58392 2:04:20, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.441500</td>\n",
              "      <td>0.378849</td>\n",
              "      <td>0.871570</td>\n",
              "      <td>0.890230</td>\n",
              "      <td>0.880801</td>\n",
              "      <td>0.931669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.333200</td>\n",
              "      <td>0.536219</td>\n",
              "      <td>0.887714</td>\n",
              "      <td>0.898599</td>\n",
              "      <td>0.893123</td>\n",
              "      <td>0.935803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.289800</td>\n",
              "      <td>0.562518</td>\n",
              "      <td>0.895829</td>\n",
              "      <td>0.898793</td>\n",
              "      <td>0.897309</td>\n",
              "      <td>0.936505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.266400</td>\n",
              "      <td>0.635242</td>\n",
              "      <td>0.887604</td>\n",
              "      <td>0.896069</td>\n",
              "      <td>0.891816</td>\n",
              "      <td>0.928471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.719996</td>\n",
              "      <td>0.896378</td>\n",
              "      <td>0.900740</td>\n",
              "      <td>0.898554</td>\n",
              "      <td>0.931981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>0.163900</td>\n",
              "      <td>0.746512</td>\n",
              "      <td>0.893264</td>\n",
              "      <td>0.900740</td>\n",
              "      <td>0.896986</td>\n",
              "      <td>0.933619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.818655</td>\n",
              "      <td>0.897307</td>\n",
              "      <td>0.901323</td>\n",
              "      <td>0.899311</td>\n",
              "      <td>0.938300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56000</td>\n",
              "      <td>0.084600</td>\n",
              "      <td>0.886056</td>\n",
              "      <td>0.891053</td>\n",
              "      <td>0.899377</td>\n",
              "      <td>0.895196</td>\n",
              "      <td>0.933385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 2] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 2] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 2] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 2] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 2] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 2] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 2] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 2] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=58392, training_loss=0.26773516840110956, metrics={'train_runtime': 7461.3421, 'train_samples_per_second': 31.304, 'train_steps_per_second': 7.826, 'total_flos': 707855506218528.0, 'train_loss': 0.26773516840110956, 'epoch': 6.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guAXbVgOqvrY"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "59N9t_AEE7PS"
      },
      "outputs": [],
      "source": [
        "# Prepare the test data for evaluation in the same format as the training data\n",
        "def calculate_results(trainer, dataset):\n",
        "\n",
        "\tpredictions, labels, _ = trainer.predict(tokenised_test)\n",
        "\tpredictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "\t# Remove the predictions for the [CLS] and [SEP] tokens\n",
        "\ttrue_predictions = [\n",
        "\t\t\t[label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "\t\t\tfor prediction, label in zip(predictions, labels)\n",
        "\t]\n",
        "\ttrue_labels = [\n",
        "\t\t\t[label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "\t\t\tfor prediction, label in zip(predictions, labels)\n",
        "\t]\n",
        "\n",
        "\t# Compute multiple metrics on the test restuls\n",
        "\tresults = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\treturn results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxd0YVXYqvrZ"
      },
      "source": [
        "# Results of 2-gram converted dataset\n",
        "From the above results we can see that the model performs decently, but the validation loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-gfpzLvqvrZ"
      },
      "source": [
        "![alt text](test1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72dHrKhqqvrZ"
      },
      "source": [
        "# Testing some larger n-grams\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Es_V-_fQqvrZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oSVAXGRqvrZ",
        "outputId": "528e2401-764c-4b14-eeac-25870a46a2f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model2 = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjKzRfFKqvrZ"
      },
      "source": [
        "# Analysis of use of n-grams for BERT model\n",
        "Above we can see the results of using n-grams on the dataset before tokenizing and training the BERT model.\n",
        "BERT uses the encoder architecture meaning that it looks at tokens in both directions (good at capturing context). For this reason, n-grams introduce a lot of unnecessary / redundant and repeated data.\\\n",
        "It is also likely that the n-grams introduced a lot of repeated data during BERT tokenization process.\n",
        "The main observation to make is the amount of tokens we end up with as a result of n-grams (much larger than without). This makes model training expensive without actually providing any improvement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3: Part two: Seperate model"
      ],
      "metadata": {
        "id": "Dzcs3xgcDRY9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52WzyEJmCJ3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}