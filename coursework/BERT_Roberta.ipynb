{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHmWij42CnB5"
      },
      "source": [
        "\n",
        "\n",
        "# Experiment #?: Comparing BERT to ROBERTA model with regular dataset and possibly larger version\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTbcROZeRRbs"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G2TtnbcCoUp"
      },
      "outputs": [],
      "source": [
        "%pip install datasets\n",
        "%pip install transformers\n",
        "%pip install spacy\n",
        "%pip install spacy-transformers\n",
        "%pip install transformers[torch]\n",
        "%pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiBzmBA_RTvB",
        "outputId": "1631da35-bba0-4f8e-effb-e9f2d5189bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version:  2.2.1+cu121\n",
            "torchtext Version:  0.17.1+cpu\n",
            "Using GPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "\n",
        "SEED = 1234\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"torchtext Version: \", torchtext.__version__)\n",
        "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "S3qO3j4uRUk0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "_oLf47NxRgQx"
      },
      "outputs": [],
      "source": [
        "def conv_label_indexes(training, valid, test ):\n",
        "\tlabel_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}\n",
        "\n",
        "\tlabel_list = []\n",
        "\tfor sample in training:\n",
        "\t\tlabel_list.append([label_encoding[tag] for tag in sample])\n",
        "\n",
        "\tval_label_list = []\n",
        "\tfor sample in valid:\n",
        "\t\tval_label_list.append([label_encoding[tag] for tag in sample])\n",
        "\n",
        "\ttest_label_list = []\n",
        "\tfor sample in test:\n",
        "\t\ttest_label_list.append([label_encoding[tag] for tag in sample])\n",
        "\treturn label_list, val_label_list, test_label_list\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Jr8sTDjiRmK9"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(tokens, tokenizer, list_name):\n",
        "    tokenized_inputs = tokenizer(tokens, truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(list_name):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "89ZHh17tRnCq"
      },
      "outputs": [],
      "source": [
        "# BERT's tokenizer returns the dataset in the form of a dictionary of lists (sentences).\n",
        "# we have to convert it into a list of dictionaries for training.\n",
        "def turn_dict_to_list_of_dict(d):\n",
        "    new_list = []\n",
        "\n",
        "    for labels, inputs in zip(d[\"labels\"], d[\"input_ids\"]):\n",
        "        entry = {\"input_ids\": inputs, \"labels\": labels}\n",
        "        new_list.append(entry)\n",
        "\n",
        "    return new_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJQfj3rVRoXx",
        "outputId": "de9b9523-c9d8-47e5-9e54-42af5f799147"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "metric = load_metric(\"seqeval\")\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "w-slJBbwRn2s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}\n",
        "inverse_label_map = {v: k for k, v in label_encoding.items()}\n",
        "\n",
        "def calculate_results(trainer, tokenised_test):\n",
        "\n",
        "    predictions, labels, _ = trainer.predict(tokenised_test)\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    textual_true_predictions = [\n",
        "        [inverse_label_map[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    textual_true_labels = [\n",
        "        [inverse_label_map[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=textual_true_predictions, references=textual_true_labels)\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70w1fp_VSibu"
      },
      "source": [
        "#Pre-process data (like in other experiments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Jh0FdivjeR3M"
      },
      "outputs": [],
      "source": [
        "training = dataset[\"train\"]\n",
        "valid = dataset[\"validation\"]\n",
        "test = dataset[\"test\"]\n",
        "training_tokens = training[\"tokens\"]\n",
        "training_labels = training[\"ner_tags\"]\n",
        "valid_tokens = valid[\"tokens\"]\n",
        "valid_labels = valid[\"ner_tags\"]\n",
        "test_tokens = test[\"tokens\"]\n",
        "test_labels = test[\"ner_tags\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "pTCc6nqrFbNK"
      },
      "outputs": [],
      "source": [
        "label_list, val_label_list, test_label_list = conv_label_indexes(training_labels, valid_labels, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "vDn39j-bFbNK"
      },
      "outputs": [],
      "source": [
        "#tokenize training set\n",
        "tokenized_train = tokenize_and_align_labels(training_tokens, rob_tokenizer, label_list)\n",
        "tokenized_val_datasets = tokenize_and_align_labels(valid_tokens, rob_tokenizer,  val_label_list)\n",
        "tokenized_test_datasets = tokenize_and_align_labels(test_tokens, rob_tokenizer, test_label_list)\n",
        "\n",
        "tokenised_train = turn_dict_to_list_of_dict(tokenized_train)\n",
        "tokenised_val = turn_dict_to_list_of_dict(tokenized_val_datasets)\n",
        "tokenised_test = turn_dict_to_list_of_dict(tokenized_test_datasets)\n",
        "\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(rob_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjv4vQfPHdZP",
        "outputId": "23098572-db97-4137-d6c4-14934a4ff966"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1072"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenised_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XmpQpvrVwqf"
      },
      "source": [
        "# Run test set against model before fine tuning\n",
        "We do this to gauge the performance of the models pre-trained state, i.e. testing how good it is for generalised use cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvP_9eJnV61L",
        "outputId": "99cb32cb-7f6a-4b11-fcfc-7898c945adf8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# Load the RoBERTa tokenizer\n",
        "rob_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
        "\n",
        "# Load the RoBERTa model for token classification with the desired number of labels\n",
        "pretrained_rob_model = AutoModelForTokenClassification.from_pretrained(\"roberta-base\", num_labels=4)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "ASUgyd6mXuQg",
        "outputId": "b2b0644e-d387-4487-f5eb-beeaf6957a69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
            "  warnings.warn(\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "\n",
        "# Training arguments (feel free to play arround with these values)\n",
        "model_name = \"roberta-base\"\n",
        "epochs = 6\n",
        "batch_size = 4\n",
        "learning_rate = 2e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"ROBERTA-finedtuned-ner\",\n",
        "    # evaluation_strategy = \"epoch\", ## Instead of focusing on loss and accuracy, we will focus on the F1 score\n",
        "    evaluation_strategy ='steps',\n",
        "    eval_steps = 100,\n",
        "    save_total_limit = 3,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.001,\n",
        "    save_steps=35000,\n",
        "    metric_for_best_model = 'f1',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    pretrained_rob_model,\n",
        "    args,\n",
        "    train_dataset=tokenised_train,\n",
        "    eval_dataset=tokenised_val,\n",
        "    data_collator = data_collator,\n",
        "    tokenizer=rob_tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "pretrain_results = calculate_results(trainer, tokenised_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGs_uNIRYMr7",
        "outputId": "111145c0-41fc-4057-b425-b6e3bcb8425b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'AC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 559},\n",
              " 'LF': {'precision': 0.04336043360433604,\n",
              "  'recall': 0.05517241379310345,\n",
              "  'f1': 0.048558421851289835,\n",
              "  'number': 290},\n",
              " 'O': {'precision': 0.807876882343207,\n",
              "  'recall': 0.925673113386424,\n",
              "  'f1': 0.8627728196518513,\n",
              "  'number': 5274},\n",
              " 'overall_precision': 0.7620974015870546,\n",
              " 'overall_recall': 0.7999346725461375,\n",
              " 'overall_f1': 0.7805577689243028,\n",
              " 'overall_accuracy': 0.7554531490015362}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrain_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK8VoDELSIC2"
      },
      "source": [
        "# Load ROBERTA Model\n",
        "Up to this point, all of my experiments have been done with the BERT model. I now want to compare the performance of BERT against roBERTA (both pre-trained and fine-tuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxRyEnLhSUON",
        "outputId": "fed681e6-01f0-4f1b-8419-5e11aa84dfa4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# Load the RoBERTa tokenizer\n",
        "rob_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
        "\n",
        "# Load the RoBERTa model for token classification with the desired number of labels\n",
        "rob_model = AutoModelForTokenClassification.from_pretrained(\"roberta-base\", num_labels=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nr1mCVhTQ8p",
        "outputId": "1145fcb4-74a0-4f6b-b585-33a4770519b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "\n",
        "# Training arguments (feel free to play arround with these values)\n",
        "model_name = \"roberta-base\"\n",
        "epochs = 6\n",
        "batch_size = 4\n",
        "learning_rate = 2e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"ROBERTA-finedtuned-ner\",\n",
        "    # evaluation_strategy = \"epoch\", ## Instead of focusing on loss and accuracy, we will focus on the F1 score\n",
        "    evaluation_strategy ='steps',\n",
        "    eval_steps = 100,\n",
        "    save_total_limit = 3,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.001,\n",
        "    save_steps=35000,\n",
        "    metric_for_best_model = 'f1',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    rob_model,\n",
        "    args,\n",
        "    train_dataset=tokenised_train,\n",
        "    eval_dataset=tokenised_val,\n",
        "    data_collator = data_collator,\n",
        "    tokenizer=rob_tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "abQ9U6NzFbNL",
        "outputId": "cf31e451-fc33-4967-8ce8-541031165cfc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1608' max='1608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1608/1608 05:10, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.241215</td>\n",
              "      <td>0.596296</td>\n",
              "      <td>0.615091</td>\n",
              "      <td>0.605548</td>\n",
              "      <td>0.911394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.189206</td>\n",
              "      <td>0.668040</td>\n",
              "      <td>0.774594</td>\n",
              "      <td>0.717382</td>\n",
              "      <td>0.929021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.198662</td>\n",
              "      <td>0.732255</td>\n",
              "      <td>0.778415</td>\n",
              "      <td>0.754630</td>\n",
              "      <td>0.940038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.178167</td>\n",
              "      <td>0.732507</td>\n",
              "      <td>0.789876</td>\n",
              "      <td>0.760110</td>\n",
              "      <td>0.938149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.285700</td>\n",
              "      <td>0.174491</td>\n",
              "      <td>0.750450</td>\n",
              "      <td>0.795606</td>\n",
              "      <td>0.772369</td>\n",
              "      <td>0.941926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.285700</td>\n",
              "      <td>0.186773</td>\n",
              "      <td>0.729515</td>\n",
              "      <td>0.790831</td>\n",
              "      <td>0.758937</td>\n",
              "      <td>0.939093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.285700</td>\n",
              "      <td>0.205523</td>\n",
              "      <td>0.757402</td>\n",
              "      <td>0.757402</td>\n",
              "      <td>0.757402</td>\n",
              "      <td>0.937362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.285700</td>\n",
              "      <td>0.202537</td>\n",
              "      <td>0.737295</td>\n",
              "      <td>0.817574</td>\n",
              "      <td>0.775362</td>\n",
              "      <td>0.940825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.285700</td>\n",
              "      <td>0.194402</td>\n",
              "      <td>0.761046</td>\n",
              "      <td>0.806113</td>\n",
              "      <td>0.782931</td>\n",
              "      <td>0.942556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.130900</td>\n",
              "      <td>0.190822</td>\n",
              "      <td>0.754260</td>\n",
              "      <td>0.803247</td>\n",
              "      <td>0.777983</td>\n",
              "      <td>0.942398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.130900</td>\n",
              "      <td>0.203557</td>\n",
              "      <td>0.751346</td>\n",
              "      <td>0.799427</td>\n",
              "      <td>0.774641</td>\n",
              "      <td>0.941926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.130900</td>\n",
              "      <td>0.219036</td>\n",
              "      <td>0.764126</td>\n",
              "      <td>0.813754</td>\n",
              "      <td>0.788159</td>\n",
              "      <td>0.944759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.130900</td>\n",
              "      <td>0.225545</td>\n",
              "      <td>0.754325</td>\n",
              "      <td>0.832856</td>\n",
              "      <td>0.791648</td>\n",
              "      <td>0.945389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.130900</td>\n",
              "      <td>0.232443</td>\n",
              "      <td>0.759483</td>\n",
              "      <td>0.841452</td>\n",
              "      <td>0.798369</td>\n",
              "      <td>0.946963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.081700</td>\n",
              "      <td>0.231837</td>\n",
              "      <td>0.758621</td>\n",
              "      <td>0.840497</td>\n",
              "      <td>0.797463</td>\n",
              "      <td>0.946963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.081700</td>\n",
              "      <td>0.233502</td>\n",
              "      <td>0.765883</td>\n",
              "      <td>0.840497</td>\n",
              "      <td>0.801457</td>\n",
              "      <td>0.946963</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1608, training_loss=0.15977293964642197, metrics={'train_runtime': 310.5928, 'train_samples_per_second': 20.709, 'train_steps_per_second': 5.177, 'total_flos': 270589932571200.0, 'train_loss': 0.15977293964642197, 'epoch': 6.0})"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " trainer.train()\n",
        "#  results = compute_results(trainer, tokenised_test)\n",
        "#  results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "y2RTZYNtFyOH",
        "outputId": "d1213204-f6c9-4ecb-a977-97c0cbfce8a7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AC': {'precision': 0.8180272108843537,\n",
              "  'recall': 0.8604651162790697,\n",
              "  'f1': 0.8387096774193549,\n",
              "  'number': 559},\n",
              " 'LF': {'precision': 0.7319277108433735,\n",
              "  'recall': 0.8379310344827586,\n",
              "  'f1': 0.7813504823151125,\n",
              "  'number': 290},\n",
              " 'O': {'precision': 0.9757564003103181,\n",
              "  'recall': 0.9539249146757679,\n",
              "  'f1': 0.9647171620325983,\n",
              "  'number': 5274},\n",
              " 'overall_precision': 0.9471691902567478,\n",
              " 'overall_recall': 0.9398987424465132,\n",
              " 'overall_f1': 0.9435199606525125,\n",
              " 'overall_accuracy': 0.9385560675883257}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " results = calculate_results(trainer, tokenised_test)\n",
        " results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3svARA3DaxMh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
