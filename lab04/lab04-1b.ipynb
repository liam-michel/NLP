{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "black-campus",
   "metadata": {},
   "source": [
    "# Lab 04 - Upgrading to an LSTM and pre-trained word embeddings\n",
    "In this lab we will experiment with LSTM, as in improvement over the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-mention",
   "metadata": {},
   "source": [
    "The lab is adopted from the [popular PyTorch sentiment analysis tutorial by bentrevett](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e38f2-1c0c-4312-abe8-d829e32ff5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install torch==1.11.0+cu113 torchdata==0.3.0 torchtext==0.12.0 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "%pip install spacy tqdm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6224f14-a178-42bc-a056-0666aa4ddadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "SEED = 1234\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"torchtext Version: \", torchtext.__version__)\n",
    "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-hands",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Load dataset\n",
    "train_data, test_data = IMDB(root=\"./\", split=(\"train\", \"test\"))\n",
    "\n",
    "# Convert to map style\n",
    "train_data = to_map_style_dataset(train_data)\n",
    "test_data = to_map_style_dataset(test_data)\n",
    "\n",
    "# Validation split\n",
    "split_ratio = 0.7  # 70/30 split\n",
    "train_samples = int(split_ratio * len(train_data))\n",
    "valid_samples = len(train_data) - train_samples\n",
    "train_data, valid_data = random_split(train_data, [train_samples, valid_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107a2ca-d5e8-4319-a62c-dd88a0ab9c3d",
   "metadata": {},
   "source": [
    "Just like last time we'll create some useful utilities for processing pipelines so we can tokenize with spaCy and get the lengths post-tokenization to use packed padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c756d8-0e5a-4142-9171-9b5fa479c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "class SpacyTokenizer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if isinstance(input, list):\n",
    "            tokens = []\n",
    "            for text in input:\n",
    "                tokens.append(self.tokenizer(text))\n",
    "            return tokens\n",
    "        elif isinstance(input, str):\n",
    "            return self.tokenizer(input)\n",
    "        raise ValueError(f\"Type {type(input)} is not supported.\")\n",
    "        \n",
    "class ToLengths(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        if isinstance(input[0], list):\n",
    "            lengths = []\n",
    "            for text in input:\n",
    "                lengths.append(len(text))\n",
    "            return lengths\n",
    "        elif isinstance(input, list):\n",
    "            return len(input)\n",
    "        raise ValueError(f\"Type {type(input)} is not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-state",
   "metadata": {},
   "source": [
    "Next is the use of pre-trained word embeddings. Instead of building a vocabulary and then generating word embeddings for each vocabulary token, with the word embeddings being initialized randomly, we will use pre-trained vectors.\n",
    "\n",
    "We get these vectors by specifying which vectors we want and passing it as an argument to `torchtext.vocab.Vectors` subclasses `vocab.GloVe`, `vocab.FastText` or `vocab.CharNGram`. `TorchText` handles downloading the vectors.\n",
    "\n",
    "Here, we'll be using the GloVe `\"6B\"` vectors. GloVe is the algorithm used to calculate the vectors, go [here](https://nlp.stanford.edu/projects/glove/) for more. `6B` indicates these vectors were trained on 6 billion tokens. We wil also specify that we want these vectors to be $100$-dimensional.\n",
    "\n",
    "You can see the other available vectors [here](https://pytorch.org/text/stable/vocab.html#pretrained-word-embeddings).\n",
    "\n",
    "The theory is that these pre-trained vectors already have words with similar semantic meaning close together in vector space, e.g. \"terrible\", \"awful\", \"dreadful\" are nearby. This gives our embedding layer a good initialization as it does not have to learn these relations from scratch.\n",
    "\n",
    "**Note**: these vectors are about 862MB, so watch out if you have a limited internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab\n",
    "\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "glove_vectors = vocab.GloVe(\n",
    "    name=\"6B\",\n",
    "    dim=100,\n",
    "    max_vectors=MAX_VOCAB_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6b23c-6f7b-4197-91df-790b48e52a05",
   "metadata": {},
   "source": [
    "So now that we have the vectors downloaded, how do we *actually* use them?\n",
    "\n",
    "In torchtext, `Vectors` objects are wrappers over a particular vocabulary (accessed with the `.stoi` member) and associated pre-trained word embeddings (accessed with the `.vectors` member). We want to decouple this into a vocabulary we can use in our data processing pipelines, and embeddings we can feed to the `Embedding` layer in our model so they are not initialized randomly.\n",
    "\n",
    "As our `<unk>` and `<pad>` token aren't in the pre-trained vocabulary it is preferable to initialize them both to all zeros to explicitly tell our model that, initially, they are irrelevant for determining sentiment. \n",
    "\n",
    "We do this by manually setting their row in the embedding matrix to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26fcef-a103-4187-9e2d-c035f2b7341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "text_vocab = vocab(glove_vectors.stoi, min_freq=0, specials=(\"<unk>\", \"<pad>\"), special_first=True)\n",
    "text_vocab.set_default_index(text_vocab[\"<unk>\"])\n",
    "\n",
    "pretrained_embeddings = glove_vectors.vectors\n",
    "pretrained_embeddings = torch.cat([\n",
    "    torch.empty(1, glove_vectors.dim).normal_(),  # unk token vector\n",
    "    torch.zeros(1, glove_vectors.dim),  # padding token vector\n",
    "    pretrained_embeddings\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb2c0b-e389-48e8-b8eb-b4e924ac8ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocab size: \", len(text_vocab))\n",
    "print(\"Pretrained vectors shape: \", pretrained_embeddings.shape)\n",
    "print(\"<unk> vector: \", pretrained_embeddings[text_vocab[\"<unk>\"]])\n",
    "print(\"<pad> vector: \", pretrained_embeddings[text_vocab[\"<pad>\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137d65fd-2988-4b2b-8184-748c0a1dcb27",
   "metadata": {},
   "source": [
    "Let's also define our label vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4c034-be55-4568-a646-4fb7c986c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "label_vocab = vocab(OrderedDict([(\"neg\", 1), (\"pos\", 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1ddeb-e46f-4e0d-90d7-1ddb6adce4d0",
   "metadata": {},
   "source": [
    "We can now define the rest of our pipelines like last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498c753-90b0-4c3e-b23b-d92634bb76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.transforms as T\n",
    "\n",
    "text_transform = T.Sequential(\n",
    "    SpacyTokenizer(),  # Tokenize\n",
    "    T.VocabTransform(text_vocab),  # Conver to vocab IDs\n",
    "    T.ToTensor(padding_value=text_vocab[\"<pad>\"]),  # Convert to tensor and pad\n",
    ")\n",
    "\n",
    "label_transform = T.Sequential(\n",
    "    T.LabelToIndex(label_vocab.get_itos()),  # Convert to integer\n",
    "    T.ToTensor(),  # Convert to tensor\n",
    ")\n",
    "\n",
    "lengths_transform = T.Sequential(\n",
    "    SpacyTokenizer(),\n",
    "    ToLengths(),\n",
    "    T.ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def collate_batch(batch):\n",
    "    labels, texts = zip(*batch)\n",
    "\n",
    "    lengths = lengths_transform(list(texts))\n",
    "    texts = text_transform(list(texts))\n",
    "    labels = label_transform(list(labels))\n",
    "\n",
    "    return labels.float().to(DEVICE), texts.to(DEVICE), lengths.cpu()\n",
    "\n",
    "def _get_dataloader(data):\n",
    "    return DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "train_dataloader = _get_dataloader(train_data)\n",
    "valid_dataloader = _get_dataloader(valid_data)\n",
    "test_dataloader = _get_dataloader(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-istanbul",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "The model features the most drastic changes.\n",
    "\n",
    "### Different RNN Architecture\n",
    "\n",
    "We'll be using an RNN architecture called a Long Short-Term Memory (LSTM). Why is an LSTM better than a standard RNN? Standard RNNs suffer from the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). LSTMs overcome this by having an extra recurrent state called a _cell_, $c$ - which can be thought of as the \"memory\" of the LSTM - and the use use multiple _gates_ which control the flow of information into and out of the memory. For more information, go [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). We can simply think of the LSTM as a function of $x_t$, $h_t$ and $c_t$, instead of just $x_t$ and $h_t$.\n",
    "\n",
    "$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n",
    "\n",
    "Thus, the model using an LSTM looks something like (with the embedding layers omitted):\n",
    "\n",
    "![](assets/sentiment2.png)\n",
    "\n",
    "The initial cell state, $c_0$, like the initial hidden state is initialized to a tensor of all zeros. The sentiment prediction is still, however, only made using the final hidden state, not the final cell state, i.e. $\\hat{y}=f(h_T)$.\n",
    "\n",
    "### Bidirectional RNN\n",
    "\n",
    "The concept behind a bidirectional RNN is simple. As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the **last to the first** (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$. \n",
    "\n",
    "In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor. \n",
    "\n",
    "We make our sentiment prediction using a concatenation of the last hidden state from the forward RNN (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward RNN (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=f(h_T^\\rightarrow, h_T^\\leftarrow)$   \n",
    "\n",
    "The image below shows a bi-directional RNN, with the forward RNN in orange, the backward RNN in green and the linear layer in silver.  \n",
    "\n",
    "![](assets/sentiment3.png)\n",
    "\n",
    "### Multi-layer RNN\n",
    "\n",
    "Multi-layer RNNs (also called *deep RNNs*) are another simple concept. The idea is that we add additional RNNs on top of the initial standard RNN, where each RNN added is another *layer*. The hidden state output by the first (bottom) RNN at time-step $t$ will be the input to the RNN above it at time step $t$. The prediction is then made from the final hidden state of the final (highest) layer.\n",
    "\n",
    "The image below shows a multi-layer unidirectional RNN, where the layer number is given as a superscript. Also note that each layer needs their own initial hidden state, $h_0^L$.\n",
    "\n",
    "![](assets/sentiment4.png)\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Although we've added improvements to our model, each one adds additional parameters. Without going into overfitting into too much detail, the more parameters you have in in your model, the higher the probability that your model will overfit (memorize the training data, causing  a low training error but high validation/testing error, i.e. poor generalization to new, unseen examples). To combat this, we use regularization. More specifically, we use a method of regularization called *dropout*. Dropout works by randomly *dropping out* (setting to 0) neurons in a layer during a forward pass. The probability that each neuron is dropped out is set by a hyperparameter and each neuron with dropout applied is considered indepenently. One theory about why dropout works is that a model with parameters dropped out can be seen as a \"weaker\" (less parameters) model. The predictions from all these \"weaker\" models (one for each forward pass) get averaged together withinin the parameters of the model. Thus, your one model can be thought of as an ensemble of weaker models, none of which are over-parameterized and thus should not overfit.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "Another addition to this model is that we are not going to learn the embedding for the `<pad>` token. This is because we want to explitictly tell our model that padding tokens are irrelevant to determining the sentiment of a sentence. This means the embedding for the pad token will remain at what it is initialized to (we initialize it to all zeros later). We do this by passing the index of our pad token as the `padding_idx` argument to the `nn.Embedding` layer.\n",
    "\n",
    "We also initialize the `nn.Embedding` layer with the `from_pretrained` function, as we'll be passing our downloaded pre-trained vectors directly to the model when initialising it. We also want the embeddings to not be trained further, so we set `freeze=True`.\n",
    "\n",
    "To use an LSTM instead of the standard RNN, we use `nn.LSTM` instead of `nn.RNN`. Also, note that the LSTM returns the `output` and a tuple of the final `hidden` state and the final `cell` state, whereas the standard RNN only returned the `output` and final `hidden` state. \n",
    "\n",
    "As the final hidden state of our LSTM has both a forward and a backward component, which will be concatenated together, the size of the input to the `nn.Linear` layer is twice that of the hidden dimension size.\n",
    "\n",
    "Implementing bidirectionality and adding additional layers are done by passing values for the `num_layers` and `bidirectional` arguments for the RNN/LSTM. \n",
    "\n",
    "Dropout is implemented by initializing an `nn.Dropout` layer (the argument is the probability of dropping out each neuron) and using it within the `forward` method after each layer we want to apply dropout to. **Note**: never use dropout on the input or output layers (`text` or `fc` in this case), you only ever want to use dropout on intermediate layers. The LSTM has a `dropout` argument which adds dropout on the connections between hidden states in one layer to hidden states in the next layer. \n",
    "\n",
    "As we are passing the lengths of our sentences to be able to use packed padded sequences, we have to add a second argument, `lengths`, to our model's `forward` function.\n",
    "\n",
    "Before we pass our embeddings to the RNN, we need to pack them like last time, which we do with `nn.utils.rnn.packed_padded_sequence`. This will cause our RNN to only process the non-padded elements of our sequence. The RNN will then return `packed_output` (a packed sequence) as well as the `hidden` and `cell` states (both of which are tensors). Without packed padded sequences, `hidden` and `cell` are tensors from the last element in the sequence, which will most probably be a pad token, however when using packed padded sequences they are both from the last non-padded element in the sequence. Note that the `lengths` argument of `packed_padded_sequence` must be a CPU tensor.\n",
    "\n",
    "The final hidden state, `hidden`, has a shape of _**[num layers * num directions, batch size, hid dim]**_. These are ordered: **[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]**. As we want the final (top) layer forward and backward hidden states, we get the top two hidden layers from the first dimension, `hidden[-2,:,:]` and `hidden[-1,:,:]`, and concatenate them together before passing them to the linear layer (after applying dropout). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True, padding_idx=pad_idx)        \n",
    "        self.rnn = nn.LSTM(pretrained_embeddings.shape[1], \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)        \n",
    "        self.fc = nn.Linear(hidden_dim * self.num_directions, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, lengths):\n",
    "        embedded = self.dropout(self.embedding(text))                   # VV note that lengths need to be on the CPU\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "\n",
    "        if self.num_directions == 2:  # if bidirectional\n",
    "            # Concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "            # and apply dropout\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-steel",
   "metadata": {},
   "source": [
    "Like before, we'll create an instance of our RNN class, with the new parameters and arguments for the number of layers, bidirectionality and dropout probability.\n",
    "\n",
    "To ensure the pre-trained vectors are loaded into the model, we pass the decoupled vectors (`pretrained_embeddings` which we created earlier) to it.\n",
    "\n",
    "Finally, we get our pad token index from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = text_vocab[\"<pad>\"]\n",
    "\n",
    "model = RNN(\n",
    "    pretrained_embeddings,\n",
    "    HIDDEN_DIM, \n",
    "    OUTPUT_DIM,\n",
    "    N_LAYERS,\n",
    "    BIDIRECTIONAL,\n",
    "    DROPOUT,\n",
    "    PAD_IDX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-greensboro",
   "metadata": {},
   "source": [
    "We'll print out the number of parameters in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-roller",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-sleeve",
   "metadata": {},
   "source": [
    "Now to training the model.\n",
    "\n",
    "The only change we'll make here is changing the optimizer from `SGD` to `Adam`. SGD updates all parameters with the same learning rate and choosing this learning rate can be tricky. `Adam` adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates. More information about `Adam` (and other optimizers) can be found [here](http://ruder.io/optimizing-gradient-descent/index.html).\n",
    "\n",
    "To change `SGD` to `Adam`, we simply change `optim.SGD` to `optim.Adam`, also note how we do not have to provide an initial learning rate for Adam as PyTorch specifies a sensibile default initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-gothic",
   "metadata": {},
   "source": [
    "The rest of the steps for training the model are unchanged.\n",
    "\n",
    "We define the criterion and place the model and criterion on the GPU (if available)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-tokyo",
   "metadata": {},
   "source": [
    "We implement the function to calculate accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-bouquet",
   "metadata": {},
   "source": [
    "We define a function for training our model. \n",
    "\n",
    "**Note**: as we are now using dropout, we must remember to use `model.train()` to ensure the dropout is \"turned on\" while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(iterator, desc=\"\\tTraining\"):\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        labels, texts, lengths = batch  # Note that this has to match the order in collate_batch\n",
    "        predictions = model(texts, lengths).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-gambling",
   "metadata": {},
   "source": [
    "Then we define a function for testing our model.\n",
    "\n",
    "**Note**: as we are now using dropout, we must remember to use `model.eval()` to ensure the dropout is \"turned off\" while evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator, desc=\"\\tEvaluation\"):\n",
    "            labels, texts, lengths = batch  # Note that this has to match the order in collate_batch\n",
    "            predictions = model(texts, lengths).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-cancer",
   "metadata": {},
   "source": [
    "And also create a nice function to tell us how long our epochs are taking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-opening",
   "metadata": {},
   "source": [
    "Finally, we train our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'} for training.\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate(model, valid_dataloader, criterion)\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-owner",
   "metadata": {},
   "source": [
    "...and get our new and vastly improved test accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-forestry",
   "metadata": {},
   "source": [
    "## User Input\n",
    "\n",
    "We can now use our model to predict the sentiment of any sentence we give it. As it has been trained on movie reviews, the sentences provided should also be movie reviews.\n",
    "\n",
    "When using a model for inference it should always be in evaluation mode (from doing `evaluate` on the test set), however we explicitly set it to avoid any risk.\n",
    "\n",
    "Our `predict_sentiment` function does a few things:\n",
    "- sets the model to evaluation mode\n",
    "- put the user input through the text processing pipeline\n",
    "- squashes the output prediction from a real number between 0 and 1 with the `sigmoid` function\n",
    "- converts the tensor holding a single value into an integer with the `item()` method\n",
    "\n",
    "We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    processed_sentence = text_transform([sentence]).to(DEVICE)\n",
    "    sentence_length = lengths_transform([sentence]).cpu()\n",
    "    prediction = torch.sigmoid(model(processed_sentence, sentence_length))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-sword",
   "metadata": {},
   "source": [
    "An example negative review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-reduction",
   "metadata": {},
   "source": [
    "An example positive review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
