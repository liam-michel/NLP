{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compliant-welsh",
   "metadata": {},
   "source": [
    "# Lab 04 - Simple sentiment analysis with an RNN \n",
    "In this lab we will experiment with different architectures of Recurrent Neural Nets (RNN), but will also use pre-trained word embeddings and several experimental setups. The Python framework for this lab relies on PyTorch.\n",
    "\n",
    "This lab is based on the [popular PyTorch sentiment analysis tutorial by bentrevett](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-perspective",
   "metadata": {},
   "source": [
    "We'll be building a machine learning model to detect sentiment (i.e. detect if a sentence is positive or negative) using PyTorch and TorchText. This will be done on movie reviews, using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "We'll be using a **recurrent neural network** (RNN) as they are commonly used in analysing sequences. An RNN takes in sequence of words, $X=\\{x_1, ..., x_T\\}$, one at a time, and produces a _hidden state_, $h$, for each word. We use the RNN _recurrently_ by feeding in the current word $x_t$ as well as the hidden state from the previous word, $h_{t-1}$, to produce the next hidden state, $h_t$. \n",
    "\n",
    "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
    "\n",
    "Once we have our final hidden state, $h_T$, (from feeding in the last word in the sequence, $x_T$) we feed it through a linear layer, $f$, (also known as a fully connected layer), to receive our predicted sentiment, $\\hat{y} = f(h_T)$.\n",
    "\n",
    "Below shows an example sentence, with the RNN predicting zero, which indicates a negative sentiment. The RNN is shown in orange and the linear layer shown in silver. Note that we use the same RNN for every word, i.e. it has the same parameters. The initial hidden state, $h_0$, is a tensor initialized to all zeros. \n",
    "\n",
    "![](assets/sentiment1.png)\n",
    "\n",
    "**Note:** some layers and steps have been omitted from the diagram, but these will be explained later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "talented-adams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
      "Collecting torch==1.11.0+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1637.0 MB)\n",
      "\u001b[2K     \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 GB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:01:13\u001b[0m^C0\u001b[0m\n",
      "\u001b[2K     \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 GB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:01:12\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/aero/miniconda3/lib/python3.10/site-packages (4.65.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m498.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m521.9 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.9/156.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.3/922.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.0/493.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy) (2.29.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy) (1.25.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.1 in /home/aero/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aero/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aero/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/aero/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aero/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/aero/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, langcodes, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.3 pydantic-core-2.16.3 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/aero/miniconda3/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/aero/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/aero/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aero/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aero/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/aero/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aero/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/aero/miniconda3/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/aero/miniconda3/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/aero/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aero/miniconda3/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install torch==1.11.0+cu113 torchdata==0.3.0 torchtext==0.12.0 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "%pip install spacy tqdm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea336d7f-04a0-49b6-83e6-53bd3fe0392c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[1;32m      4\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1234\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "SEED = 1234\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"torchtext Version: \", torchtext.__version__)\n",
    "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-developer",
   "metadata": {},
   "source": [
    "## Initialising the dataset\n",
    "\n",
    "A handy feature of TorchText is that it has support for common datasets used in natural language processing (NLP). \n",
    "\n",
    "In this cell we automatically download and load the IMDb dataset using TorchText's `datasets` package.\n",
    "\n",
    "The IMDb dataset consists of 50,000 movie reviews, each marked as being a positive or negative review. Processing on this data (such as tokenization) can be done later. Each movie review is a `(label, review)` tuple where the label is either `pos` or `neg` and the `review` is a text string.\n",
    "\n",
    "The dataset is loaded into its canonical train/test splits as `RawTextIterableDataset` objects. This means that this is an [iterable-style dataset](https://pytorch.org/docs/stable/data.html#iterable-style-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_data, test_data = IMDB(root=\"./\", split=(\"train\", \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18441ffa-d816-411c-b630-b9b2ca538a77",
   "metadata": {},
   "source": [
    "Unfortunately, that's about as far as we can go with iterable-style datasets. Iterable-style datasets in PyTorch use DataPipes to stream data from a source (in the case of the IMDB dataset and most torchtext datasets, that is text files), so we can often not know the length of the data or sample data points at will. We also can unfortunately not split the data while it is in a data stream form (although this is a feature that is currently being worked on at the time of writing, tracked on [this](https://github.com/pytorch/text/issues/1311) issue).\n",
    "\n",
    "We can, however, convert the dataset into a \"map-style dataset\" using the `to_map_style_dataset` torchtext utility. This will essentially read all the stream's data into memory and allow us to inspect the it, reason about its length, and split it to create a validation set. \n",
    "\n",
    "Usually it would be best practice to keep it as a data stream instead of loading it all into memory, which might even be impossible for certain datasets. Thankfully, the IMDB dataset's size is not prohibitively large, and we do wish to have finer control over the data for the purposes of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241cf917-f230-4562-b143-a23bf85e6679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# This might take a while\n",
    "train_data = to_map_style_dataset(train_data)\n",
    "test_data = to_map_style_dataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e74cf-c843-4a07-b83e-fe9440de133a",
   "metadata": {},
   "source": [
    "Now we can treat it as a normal dataset and do our usual data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437867eb-0f87-4a0f-95c9-aaf329579bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8b1cb-d717-434f-bfe8-95afab300de9",
   "metadata": {},
   "source": [
    "We can also check an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da5b9c-85b4-42d3-a568-dde6a7e55c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d499b8d1-e937-4613-9c0d-c36a11525af7",
   "metadata": {},
   "source": [
    "Note how samples in this data set have the label first and the data point second.\n",
    "\n",
    "To split the train set into a train and validation set, we'll use PyTorch's `random_split` utility.\n",
    "\n",
    "Note that when we initially imported PyTorch at the top of this notebook, we also set the seed using `manual_seed` to a constant, so the result of `random_split` will be reproducible in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc8ae6-1b2a-4864-ab55-e942ddb9bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "split_ratio = 0.7  # 70/30 split\n",
    "train_samples = int(split_ratio * len(train_data))\n",
    "valid_samples = len(train_data) - train_samples\n",
    "train_data, valid_data = random_split(train_data, [train_samples, valid_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eaf30b-8f6f-46c9-980e-980b0052bdf0",
   "metadata": {},
   "source": [
    "Again, we'll view how many examples are in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613c605-d4f9-4b61-b21a-b1baac19dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00d4e6-452e-4b10-bf51-7034db6ec41b",
   "metadata": {},
   "source": [
    "## Processing our data\n",
    "In the previous section we did some data exploration to understand our dataset's format. We noticed that each data sample consists of two strings: the sentiment label and the text.\n",
    "\n",
    "As we can't just feed strings into a recurrent neural network, we'll need to do some processing. Specifically:\n",
    "- We'll convert the **labels** into an integer (0 for negative, 1 for positive).\n",
    "- For the texts we'll:\n",
    "  1. Build a vocabulary.\n",
    "  2. Tokenize the text using SpaCy.\n",
    "  3. Convert each sentence into a vector of numerical vocabulary IDs.\n",
    "  4. Pad vectors to an equal length using padding tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-spain",
   "metadata": {},
   "source": [
    "### Building a vocabulary with torchtext\n",
    "In this section we'll build a _vocabulary_. This is a effectively a look up table where every unique word in your data set has a corresponding _index_ (an integer).\n",
    "\n",
    "We do this as our machine learning model cannot operate on strings, only numbers. Each _index_ is used to construct a _one-hot_ vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and dimensionality is the total number of unique words in your vocabulary, commonly denoted by $V$.\n",
    "\n",
    "![](assets/sentiment5.png)\n",
    "\n",
    "The number of unique words in our training set is over 100,000, which means that our one-hot vectors will have over 100,000 dimensions! This will make training slow and possibly won't fit onto your GPU (if you're using one). \n",
    "\n",
    "There are two ways effectively cut down our vocabulary, we can either only take the top $n$ most common words or ignore words that appear less than $m$ times. We'll do the former, only keeping the top 25,000 words.\n",
    "\n",
    "What do we do with words that appear in examples but we have cut from the vocabulary? We replace them with a special _unknown_ or `<unk>` token. For example, if the sentence was \"This film is great and I love it\" but the word \"love\" was not in the vocabulary, it would become \"This film is great and I `<unk>` it\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfd18b-37d6-48b2-b38c-370965d1bb13",
   "metadata": {},
   "source": [
    "In order to build the vocab, however, we will need to tokenize each text. Let's define a tokenizer as a PyTorch module for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8219ab3-3c94-4150-ae2b-9f99ec50d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "class SpacyTokenizer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if isinstance(input, list):\n",
    "            tokens = []\n",
    "            for text in input:\n",
    "                tokens.append(self.tokenizer(text))\n",
    "            return tokens\n",
    "        elif isinstance(input, str):\n",
    "            return self.tokenizer(input)\n",
    "        raise ValueError(f\"Type {type(input)} is not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1da89-3c4a-4cab-9ebc-b1fbc42f5f0d",
   "metadata": {},
   "source": [
    "This tokenizer will work both over a single string as well as a list of strings. The main reason we define a PyTorch `Module` for a tokenizer is so we can reuse it in our actual data processing pipeline later.\n",
    "\n",
    "Now that we have the tokenizer, we will use the `build_vocab_from_iterator` utility from torchtext to build our vocabulary. We will also use two auxiliary functions to process the train samples into tokenized texts only with `_process_texts_for_vocab()` and into just labels with `_get_labels_for_vocab()`.\n",
    "\n",
    "Additionally, note that we need to define the special `<pad>` and `<unk>` characters in the text vocabulary, and we also have to explicitly define that any unknown words should be assigned to the `<unk>` token using the `Vocab` object's `set_default_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator, vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "tokenizer = SpacyTokenizer()\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "def _process_texts_for_vocab(data):\n",
    "    for line in data:\n",
    "        yield tokenizer(line[1])\n",
    "        \n",
    "def _get_labels_for_vocab(data):\n",
    "    for line in data:\n",
    "        yield [line[0]]\n",
    "\n",
    "# This might take a while as we're tokenizing\n",
    "text_vocab = build_vocab_from_iterator(_process_texts_for_vocab(train_data), specials=('<unk>', '<pad>'), max_tokens=MAX_VOCAB_SIZE)\n",
    "label_vocab = vocab(OrderedDict([(\"neg\", 1), (\"pos\", 1)]))\n",
    "\n",
    "text_vocab.set_default_index(text_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-handle",
   "metadata": {},
   "source": [
    "Why do we only build the vocabulary on the training set? When testing any machine learning system you do not want to look at the test set in any way. We do not include the validation set as we want it to reflect the test set as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique tokens in text vocabulary: {len(text_vocab)}\")\n",
    "print(f\"Unique tokens in label vocabulary: {len(label_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-alpha",
   "metadata": {},
   "source": [
    "We can see the vocabulary directly using either of the `get_stoi()` (**s**tring **to** **i**nt) or `get_itos()` (**i**nt **to**  **s**tring) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-married",
   "metadata": {},
   "source": [
    "Regarding the `<pad>` and `<unk>` tokens you can see at the front there, one of them is a padding token and the other is the unknown token.\n",
    "\n",
    "When we feed sentences into our model, we feed a _batch_ of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size. Thus, to ensure each sentence in the batch is the same size, any shorter than the longest within the batch are padded with the `<pad>` token.\n",
    "\n",
    "![](assets/sentiment6.png)\n",
    "\n",
    "The unknown token will be used to replace any of the words encountered that are not part of our vocabulary. We can also check the labels, ensuring 0 is for negative and 1 is for positive.\n",
    "\n",
    "We can also check the labels, ensuring 0 is for negative and 1 is for positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46f67b-f44a-4f0e-bffd-450d8139e8a0",
   "metadata": {},
   "source": [
    "Unfortunately, TorchText's `Vocab` object does not give us an easy way to view the most frequent words in our vocabulary, but we can simply count them manually (although it might take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b166f-6b61-40af-8298-f4ed17c66370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "for (label, line) in train_data:\n",
    "    counter.update(tokenizer(line))\n",
    "    \n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156eaaa-61a4-4bbc-a76e-efa895e25925",
   "metadata": {},
   "source": [
    "### Defining the rest of our data processing pipelines\n",
    "We will use torchtext's `transforms` package in order to define the rest of our pipelines. These transforms are very similar to common PyTorch `nn` modules, except we can use them for NLP data processing.\n",
    "\n",
    "We'll use `transforms.Sequential` to define our pipeline in each case. For the text processing, we'll run each text through our `SpacyTokenizer`, then use `VocabTransform` to convert each tokenized sentence into a list of vocabulary IDs, and then `ToTensor` to convert this into a PyTorch Tensor. Conveniently, `ToTensor` allows us to pad all vocabulary ID sequences to the same length, we just need to give it the padding token to use. We simply query our vocabulary to get it.\n",
    "\n",
    "For the labels, we'll simply convert them to indices using `LabeltoIndex` and then convert them to a PyTorch Tensor with `ToTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c5821b-21d9-444f-84d4-2d0cadb5924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.transforms as T\n",
    "\n",
    "text_transform = T.Sequential(\n",
    "    SpacyTokenizer(),  # Tokenize\n",
    "    T.VocabTransform(text_vocab),  # Conver to vocab IDs\n",
    "    T.ToTensor(padding_value=text_vocab[\"<pad>\"]),  # Convert to tensor and pad\n",
    ")\n",
    "\n",
    "label_transform = T.Sequential(\n",
    "    T.LabelToIndex(label_vocab.get_itos()),  # Convert to integer\n",
    "    T.ToTensor(),  # Convert to tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbfd346-5361-4e3b-900b-6f2a02459e18",
   "metadata": {},
   "source": [
    "We'll also define an additional transform for the texts that will transform each text into its length *after* it gets tokenized but *before* it gets padded. These lengths will be useful when we *pack the padded sequences* later.\n",
    "\n",
    "Note that applying `text_transform` and `lengths_transform` to our texts will mean that we will tokenize them twice, which is somewhat inefficient, but we will do so anyway for the sake of simplicity. In your implementations, you might want to consider having a \"shared\" pipeline that handles tokenization / vocabulary transformation and then having two separate pipelines that do further processing over that (one to extract the lengths and one to pad each tensor, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfbce3-7fb2-43a3-b1c6-1597c59c7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToLengths(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        if isinstance(input[0], list):\n",
    "            lengths = []\n",
    "            for text in input:\n",
    "                lengths.append(len(text))\n",
    "            return lengths\n",
    "        elif isinstance(input, list):\n",
    "            return len(input)\n",
    "        raise ValueError(f\"Type {type(input)} is not supported.\")\n",
    "        \n",
    "lengths_transform = T.Sequential(\n",
    "    SpacyTokenizer(),\n",
    "    ToLengths(),\n",
    "    T.ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c31fb-df56-41b5-bee0-ea4e7c05a088",
   "metadata": {},
   "source": [
    "### Understanding the processing being done\n",
    "Before moving on, let's examine what exactly each step in our processing pipelines will do to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80b8a3-3a64-46a9-a8dd-bf909b799f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_label, sample_text = train_data[0]\n",
    "\n",
    "print(f\"Text before any processing: {sample_text}\")\n",
    "print(f\"Label before any processing: {sample_label}\\n\")\n",
    "\n",
    "# Text Processing Pipeline\n",
    "tokenizer = SpacyTokenizer()\n",
    "sample_text = tokenizer(sample_text)\n",
    "print(f\"Text after Tokenizer: {sample_text}\\n\")\n",
    "\n",
    "vocab_transform = T.VocabTransform(text_vocab)\n",
    "sample_text = vocab_transform(sample_text)\n",
    "print(f\"Text after Vocab Transform: {sample_text}\\n\")\n",
    "\n",
    "tensor_transform = T.ToTensor(padding_value=text_vocab[\"<pad>\"])\n",
    "sample_text = tensor_transform(sample_text)\n",
    "print(f\"Text after Tensor Transform: {sample_text}\\n\")\n",
    "\n",
    "# Label Processing Pipeline\n",
    "print(f\"Label after label transform: {label_transform([sample_label])}\\n\")\n",
    "\n",
    "# Length Processing Pipeline\n",
    "print(f\"Text after length transform: {lengths_transform([train_data[0][1]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421153cd-82e3-4c67-94d0-4dfcf0ef0f21",
   "metadata": {},
   "source": [
    "Do note that when we take two texts of differing sizes, they will be padded after going through the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98820f-4104-41c7-86ca-0bed7ae55429",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_labels, sample_texts = zip(train_data[0], train_data[1])\n",
    "\n",
    "processed_sample_texts = text_transform(list(sample_texts))\n",
    "lengths = lengths_transform(list(sample_texts))\n",
    "diff = abs(lengths[0] - lengths[1]) + 5\n",
    "\n",
    "print(f\"Padding vocabulary index: {text_vocab['<pad>']}\")\n",
    "\n",
    "print(\"Respective text lengths after tokenization: \", lengths)\n",
    "print(\"Tensor shape after text processing: \", processed_sample_texts.shape)\n",
    "print(f\"Last {diff} characters of text 0 after processing:\\n\", processed_sample_texts[0][-diff:])\n",
    "print(f\"Last {diff} characters of text 1 after processing:\\n\", processed_sample_texts[1][-diff:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f720e8-4c50-4c1c-970e-92e1e6f3567f",
   "metadata": {},
   "source": [
    "Notice how one text is shorter, but after the `ToTensor` transformation both texts end up with the same sentence length, and how the shorter text was padded with the padding token from the vocabulary for its missing characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3e175-dfe5-44b0-8b07-03fa1a78473d",
   "metadata": {},
   "source": [
    "### The DataLoader\n",
    "Now we've got all our originally planned processing set up and ready to go. The final step is to put our data into a PyTorch `DataLoader`. The `DataLoader` will help us iterate over the data in batches ofexamples at each iteartion.\n",
    "\n",
    "PyTorch's `DataLoader` provides quite a few features to let us iterate over data, but it doesn't do any processing on its own. We need to define a `collate_batch` function where we explicitly define what processing steps each batch of data will need to go through when it goes through the data loader. In essence, we'll just use that function to feed our data through the pipelines we created previously.\n",
    "\n",
    "We also want to place the tensors returned on the GPU (if you're using one). PyTorch handles this using `torch.device`, which we can then pass our tensors to in `collate_batch`. Finally, note that we convert the labels to float values so we can compute loss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def collate_batch(batch):\n",
    "    labels, texts = zip(*batch)\n",
    "\n",
    "    lengths = lengths_transform(list(texts))\n",
    "    texts = text_transform(list(texts))\n",
    "    labels = label_transform(list(labels))\n",
    "\n",
    "    return labels.float().to(DEVICE), texts.to(DEVICE), lengths.cpu()\n",
    "\n",
    "def _get_dataloader(data):\n",
    "    return DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "train_dataloader = _get_dataloader(train_data)\n",
    "valid_dataloader = _get_dataloader(valid_data)\n",
    "test_dataloader = _get_dataloader(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b1e41-2697-46f2-af10-067389ed684b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary\n",
    "- We imported the `IMDB` dataset from torchtext and converted it from an iterable-style data stream data set to a map-style data set with `to_map_style_dataset`.\n",
    "- We split it further to obtain a validation set using `random_split`.\n",
    "- We defined a Tokenizer using SpaCy as a PyTorch Module.\n",
    "- We used `build_vocab_from_iterator` and some auxiliary functions to create our vocabulary.\n",
    "- We used `torchtext.transforms` to define processing pipelines.\n",
    "- We used `torch.utils.data.DataLoader` as well as an auxiliary function to put our data through our data pipelines and into batches.\n",
    "\n",
    "That was a lot, but we are *finally* ready to put our data through our model!\n",
    "\n",
    "Unfortunately, torchtext is still a very young project and a lot of reference material still uses its old legacy API. When trying to look deeper into torchtext, it's recommended to make sure you're checking its [latest documentation](https://pytorch.org/text/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-stress",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "The next stage is building the model that we'll eventually train and evaluate. \n",
    "\n",
    "There is a small amount of boilerplate code when creating models in PyTorch, note how our `RNN` class is a sub-class of `nn.Module` and the use of `super`.\n",
    "\n",
    "Within the `__init__` we define the _layers_ of the module. Our three layers are an _embedding_ layer, our RNN, and a _linear_ layer. All layers have their parameters initialized to random values, unless explicitly specified.\n",
    "\n",
    "The embedding layer is used to transform our sparse word representations (sparse as most of the elements are 0) into dense embedding vectors (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see [here](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/).\n",
    "\n",
    "The RNN layer is our RNN which takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n",
    "\n",
    "![](assets/sentiment7.png)\n",
    "\n",
    "Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension.\n",
    "\n",
    "The `forward` method is called when we feed examples into our model.\n",
    "\n",
    "Each batch, `texts`, is a tensor of size **[batch_size, batch_sentence_length]**. That is a batch of sentences, each having each word converted into its vocabulary index in our previous processing steps. The act of converting a list of tokens into a list of indexes is commonly called *numericalizing*. The `Embedding` layer will take care of converting these indices into one-hot vectors. Note that `batch_sentence_length` is the length of the largest sentence in the batch.\n",
    "\n",
    "The input batch is then passed through the embedding layer to get `embedded`, which gives us a dense vector representation of our sentences. `embedded` is a tensor of size **[batch size, batch_sentence_length, embedding dim]**.\n",
    "\n",
    "Each batch, we also have access to `lengths` which is a tensor of the lengths of each sentence before it was padded to be of size `batch_sentence_length`. We will use these lengths to essentially remove the padding present in each sentence into the batch, and combine all sentences into one large vector that will get fed through the RNN. This is mainly a performance optimization but it is generally good practice. Note that even if we are combining all our input into one continuous tensor, the RNN will still output results for individual sentences, the main thing we gain is that we don't waste computation time processing the padding present in the original tensors.\n",
    "\n",
    "`embedded` is then fed into the RNN. In some frameworks you must feed the initial hidden state, $h_0$, into the RNN, however in PyTorch, if no initial hidden state is passed as an argument it defaults to a tensor of all zeros.\n",
    "\n",
    "The RNN returns 2 tensors, `output` normally of size **[batch size, batch_sentence length, hidden dim]** and `hidden` of size **[1, batch size, hidden dim]**. `output` is the concatenation of the hidden state from every time step, whereas `hidden` is simply the final hidden state. Since we're using packed padded sequences, `output` will also be a packed padded sequence, so its size will be slightly different and overall smaller the normal size.\n",
    "\n",
    "Finally, we feed the last hidden state, `hidden`, through the linear layer, `fc`, to produce a prediction. Note the `squeeze` method, which is used to remove a dimension of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, texts, lengths):\n",
    "        embedded = self.embedding(texts)                          # VV note that lengths need to be on the CPU\n",
    "        embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-village",
   "metadata": {},
   "source": [
    "We now create an instance of our RNN class. \n",
    "\n",
    "The input dimension is the dimension of the one-hot vectors, which is equal to the vocabulary size. \n",
    "\n",
    "The embedding dimension is the size of the dense word vectors. This is usually around 50-250 dimensions, but depends on the size of the vocabulary.\n",
    "\n",
    "The hidden dimension is the size of the hidden states. This is usually around 100-500 dimensions, but also depends on factors such as on the vocabulary size, the size of the dense vectors and the complexity of the task.\n",
    "\n",
    "The output dimension is usually the number of classes, however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, i.e. a single scalar real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(text_vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-exemption",
   "metadata": {},
   "source": [
    "Let's also create a function that will tell us how many trainable parameters our model has so we can compare the number of parameters across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-convenience",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-contamination",
   "metadata": {},
   "source": [
    "Now we'll set up the training and then train the model.\n",
    "\n",
    "First, we'll create an optimizer. This is the algorithm we use to update the parameters of the module. Here, we'll use _stochastic gradient descent_ (SGD). The first argument is the parameters will be updated by the optimizer, the second is the learning rate, i.e. how much we'll change the parameters by when we do a parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-memorabilia",
   "metadata": {},
   "source": [
    "Next, we'll define our loss function. In PyTorch this is commonly called a criterion. \n",
    "\n",
    "The loss function here is _binary cross entropy with logits_. \n",
    "\n",
    "Our model currently outputs an unbound real number. As our labels are either 0 or 1, we want to restrict the predictions to a number between 0 and 1. We do this using the _sigmoid_ or _logit_ functions. \n",
    "\n",
    "We then use this this bound scalar to calculate the loss using binary cross entropy. \n",
    "\n",
    "The `BCEWithLogitsLoss` criterion carries out both the sigmoid and the binary cross entropy steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-norman",
   "metadata": {},
   "source": [
    "Using `.to`, we can place the model and the criterion on the GPU (if we have one). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-massachusetts",
   "metadata": {},
   "source": [
    "Our criterion function calculates the loss, however we have to write our function to calculate the accuracy. \n",
    "\n",
    "This function first feeds the predictions through a sigmoid layer, squashing the values between 0 and 1, we then round them to the nearest integer. This rounds any value greater than 0.5 to 1 (a positive sentiment) and the rest to 0 (a negative sentiment).\n",
    "\n",
    "We then calculate how many rounded predictions equal the actual labels and average it across the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-population",
   "metadata": {},
   "source": [
    "The `train` function iterates over all examples, one batch at a time. \n",
    "\n",
    "`model.train()` is used to put the model in \"training mode\", which turns on _dropout_ and _batch normalization_. Although we aren't using them in this model, it's good practice to include it.\n",
    "\n",
    "For each batch, we first zero the gradients. Each parameter in a model has a `grad` attribute which stores the gradient calculated by the `criterion`. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
    "\n",
    "We then feed the batch of sentences and their original lengths, `texts` and `lengths` accordingly, into the model. Note, you do not need to do `model.forward(texts, lengths)`, simply calling the model works. The `squeeze` is needed as the predictions are initially size _**[batch size, 1]**_, and we need to remove the dimension of size 1 as PyTorch expects the predictions input to our criterion function to be of size _**[batch size]**_.\n",
    "\n",
    "The loss and accuracy are then calculated using our predictions and the labels, `labels`, with the loss being averaged over all examples in the batch.\n",
    "\n",
    "We calculate the gradient of each parameter with `loss.backward()`, and then update the parameters using the gradients and optimizer algorithm with `optimizer.step()`.\n",
    "\n",
    "The loss and accuracy is accumulated across the epoch, the `.item()` method is used to extract a scalar from a tensor which only contains a single value.\n",
    "\n",
    "Finally, we return the loss and accuracy, averaged across the epoch. The `len` of an iterator is the number of batches in the iterator.\n",
    "\n",
    "You may recall that we converted the labels to float in `collate_batch()`. This is because `ToTensor` sets tensors to be `LongTensor`s by default, however our criterion expects both inputs to be `FloatTensor`s. The alternative method of doing this would be to do the conversion inside the `train` function by passing `labels.float()` instad of `labels` to the criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(iterator, desc=\"\\tTraining\"):\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        labels, texts, lengths = batch  # Note that this has to match the order in collate_batch\n",
    "        predictions = model(texts, lengths).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-wagon",
   "metadata": {},
   "source": [
    "`evaluate` is similar to `train`, with a few modifications as you don't want to update the parameters when evaluating.\n",
    "\n",
    "`model.eval()` puts the model in \"evaluation mode\", this turns off _dropout_ and _batch normalization_. Again, we are not using them in this model, but it is good practice to include them.\n",
    "\n",
    "No gradients are calculated on PyTorch operations inside the `with no_grad()` block. This causes less memory to be used and speeds up computation.\n",
    "\n",
    "The rest of the function is the same as `train`, with the removal of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as we do not update the model's parameters when evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator, desc=\"\\tEvaluation\"):\n",
    "            labels, texts, lengths = batch  # Note that this has to match the order in collate_batch\n",
    "            predictions = model(texts, lengths).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-arena",
   "metadata": {},
   "source": [
    "We'll also create a function to tell us how long an epoch takes to compare training times between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-buffer",
   "metadata": {},
   "source": [
    "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the training and validation sets.\n",
    "\n",
    "At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'} for training.\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate(model, valid_dataloader, criterion)\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-making",
   "metadata": {},
   "source": [
    "You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we'll improve in the next notebook.\n",
    "\n",
    "Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-albany",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook, the improvements we will make are:\n",
    "- pre-trained word embeddings\n",
    "- different RNN architectures\n",
    "- bidirectional RNN\n",
    "- multi-layer RNN\n",
    "- regularization\n",
    "- a different optimizer\n",
    "\n",
    "This will allow us to achieve ~84% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
