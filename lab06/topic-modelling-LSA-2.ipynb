{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "limiting-cooling",
   "metadata": {},
   "source": [
    "# Lab 06 - Topic Modelling\n",
    "In this lab we will look into building topic models, but will also examine dimensionality reduction and other relevant subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-institute",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)\n",
    "Based on: [Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-teens",
   "metadata": {},
   "source": [
    "### Data reading and inspection\n",
    "Let’s load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "static-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-console",
   "metadata": {},
   "source": [
    "We will use the ’20 Newsgroup’ dataset from sklearn.\n",
    "\n",
    "OPTIONALY: You can also download the dataset [here](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups), if you want to look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "restricted-corruption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-financing",
   "metadata": {},
   "source": [
    "Let's look at the 20 labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tracked-devices",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-estimate",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "To start with, we will try to clean our text data as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1d08b",
   "metadata": {},
   "source": [
    "### Challenge 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "measured-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "# TODO: Remove everything except alphabets\n",
    "news_df['clean_doc'] = ...\n",
    "\n",
    "# TODO: Remove short words (words with len < 4) from `clean_doc` column.\n",
    "news_df['clean_doc'] = ...\n",
    "\n",
    "# TODO: Make all text lowercase\n",
    "news_df['clean_doc'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-opportunity",
   "metadata": {},
   "source": [
    "Tokenise and remove the stop-words... eventually we will stitch the text back together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20496337",
   "metadata": {},
   "source": [
    "### Challenge 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equal-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# TODO: Tokenization\n",
    "tokenized_doc = ...\n",
    "\n",
    "# TODO: Remove stop-words\n",
    "tokenized_doc = ...\n",
    "\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-naples",
   "metadata": {},
   "source": [
    "### TFIDF Document-Term Matrix\n",
    "This is the first step towards topic modeling. We will use sklearn’s TfidfVectorizer to create a document-term matrix with 1,000 terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dec859",
   "metadata": {},
   "source": [
    "### Challenge 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compressed-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # keep top 1000 terms \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "# TODO: Use fit_transform func to transform `clean_doc` with TfidfVectorizer\n",
    "X = ...\n",
    "\n",
    "X.shape # check shape of the document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-ferry",
   "metadata": {},
   "source": [
    "We could have used all the terms to create this matrix but that would need quite a lot of computation time and resources. Hence, we have restricted the number of features to 1,000. If you have the computational power, I suggest trying out all the terms.\n",
    "\n",
    "### Topic Modeling\n",
    "The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn’s TruncatedSVD to perform the task of matrix decomposition.\n",
    "\n",
    "Since the data comes from 20 different newsgroups, let’s try to have 20 topics for our text data. The number of topics can be specified by using the n_components parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "possible-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "# TODO: Fit svd_model on X\n",
    "...\n",
    "\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-practitioner",
   "metadata": {},
   "source": [
    "The components of svd_model are our topics, and we can access them using svd_model.components_. Finally, let’s print a few most important words in each of the 20 topics and see how our model has done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sustained-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0],\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-opportunity",
   "metadata": {},
   "source": [
    "### Topics Visualization\n",
    "To find out how distinct our topics are, we should visualize them. Of course, we cannot visualize more than 3 dimensions, but there are techniques like t-SNE which can help us visualize high dimensional data into lower dimensions. Here we will use a relatively new technique called UMAP (Uniform Manifold Approximation and Projection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "meaning-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn\n",
    "import umap.umap_ as umap\n",
    "\n",
    "X_topics = svd_model.fit_transform(X)\n",
    "embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "            c = dataset.target,\n",
    "            s = 10, # size\n",
    "            edgecolor='none')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
