{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "happy-sigma",
   "metadata": {},
   "source": [
    "# Lab 01 - Tokenisation and basic feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-supervision",
   "metadata": {},
   "source": [
    "This lab is an introduction to some basic test processing that will give us the first impression of the challenges in translating text into meaningful feature vector representations.\n",
    "\n",
    "So let's first set some text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stopped-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\" Welcome to the Natural Language Processing lab!\n",
    "               We'll learn many things in this no 1 lab, so we will take it easy.\n",
    "               Natural Language Processing is fun.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86574662",
   "metadata": {},
   "source": [
    "Let's also download some libraries we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76028587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/va00282/Library/Python/3.9/lib/python/site-packages (1.23.4)\n",
      "Requirement already satisfied: pandas in /Users/va00282/Library/Python/3.9/lib/python/site-packages (1.5.1)\n",
      "Requirement already satisfied: sklearn in /Users/va00282/Library/Python/3.9/lib/python/site-packages (0.0)\n",
      "Requirement already satisfied: nltk in /Users/va00282/Library/Python/3.9/lib/python/site-packages (3.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/va00282/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/va00282/Library/Python/3.9/lib/python/site-packages (from pandas) (2022.5)\n",
      "Requirement already satisfied: scikit-learn in /Users/va00282/Library/Python/3.9/lib/python/site-packages (from sklearn) (1.2.0)\n",
      "Requirement already satisfied: joblib in /Users/va00282/Library/Python/3.9/lib/python/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /Users/va00282/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/va00282/Library/Python/3.9/lib/python/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/va00282/Library/Python/3.9/lib/python/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/va00282/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/va00282/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas sklearn nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da2c66",
   "metadata": {},
   "source": [
    "## Using native Python functions for tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-perry",
   "metadata": {},
   "source": [
    "Let's try to split the text based on spaces using the built in string function `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "careful-cleaning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab!',\n",
       " \"We'll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab,',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-brain",
   "metadata": {},
   "source": [
    "You will observe that some words are not well separated from punctuation and contain some appended onto them.\n",
    "So we need to find a way to remove those characters... but, before we do that, let's see how we can create a quick feature vector first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regional-services",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'Language',\n",
       " 'Natural',\n",
       " 'Processing',\n",
       " \"We'll\",\n",
       " 'Welcome',\n",
       " 'easy.',\n",
       " 'fun.',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'lab!',\n",
       " 'lab,',\n",
       " 'learn',\n",
       " 'many',\n",
       " 'no',\n",
       " 'so',\n",
       " 'take',\n",
       " 'the',\n",
       " 'things',\n",
       " 'this',\n",
       " 'to',\n",
       " 'we',\n",
       " 'will']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sentence.split()  # splitting based on spaces\n",
    "vocab = sorted(set(tokens))  # sorting and removing duplicates by using set()\n",
    "vocab  # just printing the vocab so we can look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-syndication",
   "metadata": {},
   "source": [
    "We can see that the sorted list has the numbers first, followed by capital and then lower case letters (all alphabetically sorted). We also see that repeating words appear only once in our vocabulary list. Let's compare the size of the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "periodic-typing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 27\n",
      "Vocab: 24\n"
     ]
    }
   ],
   "source": [
    "tokens_len = len(tokens)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "print(f\"Tokens: {tokens_len}\")\n",
    "print(f\"Vocab: {vocab_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-private",
   "metadata": {},
   "source": [
    "Let's try and print the matrix of tokens against vocabulary. We will use the numpy lib for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ultimate-litigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.zeros((tokens_len, vocab_len), int)\n",
    "for i, token in enumerate(tokens):\n",
    "    matrix[i, vocab.index(token)] = 1\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-beach",
   "metadata": {},
   "source": [
    "It's not easy to see, but the second, third and fourth columns have the value of 1 in two rows, whereas the rest only take the value of 1 in a single row. To make it a little more readable, we could use Pandas and DataFrame! Both Pandas and NumPy are very useful libs that we will use many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "realistic-wallace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>Language</th>\n",
       "      <th>Natural</th>\n",
       "      <th>Processing</th>\n",
       "      <th>We'll</th>\n",
       "      <th>Welcome</th>\n",
       "      <th>easy.</th>\n",
       "      <th>fun.</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>...</th>\n",
       "      <th>many</th>\n",
       "      <th>no</th>\n",
       "      <th>so</th>\n",
       "      <th>take</th>\n",
       "      <th>the</th>\n",
       "      <th>things</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1  Language  Natural  Processing  We'll  Welcome  easy.  fun.  in  is  \\\n",
       "0   0         0        0           0      0        1      0     0   0   0   \n",
       "1   0         0        0           0      0        0      0     0   0   0   \n",
       "2   0         0        0           0      0        0      0     0   0   0   \n",
       "3   0         0        1           0      0        0      0     0   0   0   \n",
       "4   0         1        0           0      0        0      0     0   0   0   \n",
       "5   0         0        0           1      0        0      0     0   0   0   \n",
       "6   0         0        0           0      0        0      0     0   0   0   \n",
       "7   0         0        0           0      1        0      0     0   0   0   \n",
       "8   0         0        0           0      0        0      0     0   0   0   \n",
       "9   0         0        0           0      0        0      0     0   0   0   \n",
       "10  0         0        0           0      0        0      0     0   0   0   \n",
       "11  0         0        0           0      0        0      0     0   1   0   \n",
       "12  0         0        0           0      0        0      0     0   0   0   \n",
       "13  0         0        0           0      0        0      0     0   0   0   \n",
       "14  1         0        0           0      0        0      0     0   0   0   \n",
       "15  0         0        0           0      0        0      0     0   0   0   \n",
       "16  0         0        0           0      0        0      0     0   0   0   \n",
       "17  0         0        0           0      0        0      0     0   0   0   \n",
       "18  0         0        0           0      0        0      0     0   0   0   \n",
       "19  0         0        0           0      0        0      0     0   0   0   \n",
       "20  0         0        0           0      0        0      0     0   0   0   \n",
       "21  0         0        0           0      0        0      1     0   0   0   \n",
       "22  0         0        1           0      0        0      0     0   0   0   \n",
       "23  0         1        0           0      0        0      0     0   0   0   \n",
       "24  0         0        0           1      0        0      0     0   0   0   \n",
       "25  0         0        0           0      0        0      0     0   0   1   \n",
       "26  0         0        0           0      0        0      0     1   0   0   \n",
       "\n",
       "    ...  many  no  so  take  the  things  this  to  we  will  \n",
       "0   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "1   ...     0   0   0     0    0       0     0   1   0     0  \n",
       "2   ...     0   0   0     0    1       0     0   0   0     0  \n",
       "3   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "4   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "5   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "6   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "7   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "8   ...     0   0   0     0    0       0     0   0   0     0  \n",
       "9   ...     1   0   0     0    0       0     0   0   0     0  \n",
       "10  ...     0   0   0     0    0       1     0   0   0     0  \n",
       "11  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "12  ...     0   0   0     0    0       0     1   0   0     0  \n",
       "13  ...     0   1   0     0    0       0     0   0   0     0  \n",
       "14  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "15  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "16  ...     0   0   1     0    0       0     0   0   0     0  \n",
       "17  ...     0   0   0     0    0       0     0   0   1     0  \n",
       "18  ...     0   0   0     0    0       0     0   0   0     1  \n",
       "19  ...     0   0   0     1    0       0     0   0   0     0  \n",
       "20  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "21  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "22  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "23  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "24  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "25  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "26  ...     0   0   0     0    0       0     0   0   0     0  \n",
       "\n",
       "[27 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-barrel",
   "metadata": {},
   "source": [
    "Now this is a lot more clear and if we wanted we could carry on making it look nicer.\n",
    "\n",
    "Let's now carry on building the bag of words (BoW) -\n",
    "- *Bags of words (BoW) are a basic representation technique used in NLP.*\n",
    "- *Its main purpose is to convert textual data into a numerical format that can be used for further analysis.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anonymous-anaheim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1),\n",
       " ('Language', 1),\n",
       " ('Natural', 1),\n",
       " ('Processing', 1),\n",
       " (\"We'll\", 1),\n",
       " ('Welcome', 1),\n",
       " ('easy.', 1),\n",
       " ('fun.', 1),\n",
       " ('in', 1),\n",
       " ('is', 1),\n",
       " ('it', 1),\n",
       " ('lab!', 1),\n",
       " ('lab,', 1),\n",
       " ('learn', 1),\n",
       " ('many', 1),\n",
       " ('no', 1),\n",
       " ('so', 1),\n",
       " ('take', 1),\n",
       " ('the', 1),\n",
       " ('things', 1),\n",
       " ('this', 1),\n",
       " ('to', 1),\n",
       " ('we', 1),\n",
       " ('will', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = {token: 1 for token in tokens}  # setting this up as a dictionary\n",
    "sorted(bow.items())  # lets print it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-palestine",
   "metadata": {},
   "source": [
    "Since bow is a dictionary, we see that there are no duplicate words.\n",
    "\n",
    "Pandas also has a more efficient form of a dictionary called `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adequate-aquatic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Welcome</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "      <th>Natural</th>\n",
       "      <th>Language</th>\n",
       "      <th>Processing</th>\n",
       "      <th>lab!</th>\n",
       "      <th>We'll</th>\n",
       "      <th>learn</th>\n",
       "      <th>many</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>lab,</th>\n",
       "      <th>so</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>take</th>\n",
       "      <th>it</th>\n",
       "      <th>easy.</th>\n",
       "      <th>is</th>\n",
       "      <th>fun.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Welcome  to  the  Natural  Language  Processing  lab!  We'll  learn  \\\n",
       "sent        1   1    1        1         1           1     1      1      1   \n",
       "\n",
       "      many  ...  1  lab,  so  we  will  take  it  easy.  is  fun.  \n",
       "sent     1  ...  1     1   1   1     1     1   1      1   1     1  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in tokens])), columns=[\"sent\"]).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "senior-shareware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Welcome</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "      <th>Natural</th>\n",
       "      <th>Language</th>\n",
       "      <th>Processing</th>\n",
       "      <th>lab!</th>\n",
       "      <th>We'll</th>\n",
       "      <th>learn</th>\n",
       "      <th>many</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>lab,</th>\n",
       "      <th>so</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>take</th>\n",
       "      <th>it</th>\n",
       "      <th>easy.</th>\n",
       "      <th>is</th>\n",
       "      <th>fun.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Welcome  to  the  Natural  Language  Processing  lab!  We'll  learn  \\\n",
       "sent0        1   1    1        1         1           1     1      0      0   \n",
       "sent1        0   0    0        0         0           0     0      1      1   \n",
       "sent2        0   0    0        1         1           1     0      0      0   \n",
       "\n",
       "       many  ...  1  lab,  so  we  will  take  it  easy.  is  fun.  \n",
       "sent0     0  ...  0     0   0   0     0     0   0      0   0     0  \n",
       "sent1     1  ...  1     1   1   1     1     1   1      1   0     0  \n",
       "sent2     0  ...  0     0   0   0     0     0   0      0   1     1  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {}\n",
    "for i, sent in enumerate(sentence.split('\\n')):\n",
    "    corpus[f\"sent{i}\"] = dict((tok, 1) for tok in sent.split())\n",
    "\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-rachel",
   "metadata": {},
   "source": [
    "Now we see how we managed to build feature vectors for the three sentences we originally had. Now let's do a Dot Product calculation.\n",
    "\n",
    "- *In the context of measuring similarity, the dot product is often used to quantify the **similarity** between two vectors.*\n",
    "- *The **higher** the dot product, the more **similar** the vectors are considered to be.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "computational-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of sent0 from sent1: 0 and dot product of sent0 from sent1: 3\n"
     ]
    }
   ],
   "source": [
    "df = df.T\n",
    "print(f\"Dot product of sent0 from sent1: {df.sent0.dot(df.sent1)} and dot product of sent0 from sent2: {df.sent0.dot(df.sent2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-pavilion",
   "metadata": {},
   "source": [
    "As we see from the results, the higher the dot product to more similar the vectors are... so given that only the first and last sentence have some common words, we see that this comes back as 3, where as the two sentences who have nothing in common come bak as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-realtor",
   "metadata": {},
   "source": [
    "We can improve our vocabulary now if we were to remove all other punctuation. Let's do that with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "saving-fifty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab',\n",
       " \"We'll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(r\"[-\\s.,;!?]+\", sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710aa204",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "Although this seems to be great... you might still have issues with different characters that are not anticipated. So we usually use an existing NLP related tokeniser to do this job. Let's try the NLTK lib.\n",
    "\n",
    "NLTK also supports regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "prime-limit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab',\n",
       " '!',\n",
       " 'We',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy',\n",
       " '.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|$[0-9.]+|\\S+\")\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-prototype",
   "metadata": {},
   "source": [
    "but there are other more specialised tokenisers, such as the *Treebank Word Tokenizer*:\n",
    "\n",
    "\n",
    "- *It is specifically designed to tokenize text in a manner similar to the **Penn Treebank Tokenization conventions**.*\n",
    "- *The Penn Treebank is a **large corpus** of English text, and the tokenization conventions used in its processing have become a standard for various NLP tasks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rapid-dictionary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'lab',\n",
       " '!',\n",
       " 'We',\n",
       " \"'ll\",\n",
       " 'learn',\n",
       " 'many',\n",
       " 'things',\n",
       " 'in',\n",
       " 'this',\n",
       " 'no',\n",
       " '1',\n",
       " 'lab',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'it',\n",
       " 'easy.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'fun',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-graphic",
   "metadata": {},
   "source": [
    "For now let's use the regular expression special word pattern `\\w`, so we can control what the tokeniser does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "south-engine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'to', 'the', 'Natural', 'Language', 'Processing', 'lab', 'We', 'll', 'learn', 'many', 'things', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'Natural', 'Language', 'Processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-category",
   "metadata": {},
   "source": [
    "At the point you could try out different other tokenisers from other libraries and see if there are any differences.\n",
    "\n",
    "We will now calculate the 2-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "guilty-export",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Welcome', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'Natural'),\n",
       " ('Natural', 'Language'),\n",
       " ('Language', 'Processing'),\n",
       " ('Processing', 'lab'),\n",
       " ('lab', 'We'),\n",
       " ('We', 'll'),\n",
       " ('ll', 'learn'),\n",
       " ('learn', 'many'),\n",
       " ('many', 'things'),\n",
       " ('things', 'in'),\n",
       " ('in', 'this'),\n",
       " ('this', 'no'),\n",
       " ('no', '1'),\n",
       " ('1', 'lab'),\n",
       " ('lab', 'so'),\n",
       " ('so', 'we'),\n",
       " ('we', 'will'),\n",
       " ('will', 'take'),\n",
       " ('take', 'it'),\n",
       " ('it', 'easy'),\n",
       " ('easy', 'Natural'),\n",
       " ('Natural', 'Language'),\n",
       " ('Language', 'Processing'),\n",
       " ('Processing', 'is'),\n",
       " ('is', 'fun')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-present",
   "metadata": {},
   "source": [
    "and 3-grams:\n",
    "\n",
    "These *$n$-grams* are a contiguous sequence of $n$ items from a given sample of text or speech. They can be used to **identify** common phrases or expressions and can be valuable in tasks like language modeling, where predicting the next word is based on the preceding one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "imperial-retreat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Welcome', 'to', 'the'),\n",
       " ('to', 'the', 'Natural'),\n",
       " ('the', 'Natural', 'Language'),\n",
       " ('Natural', 'Language', 'Processing'),\n",
       " ('Language', 'Processing', 'lab'),\n",
       " ('Processing', 'lab', 'We'),\n",
       " ('lab', 'We', 'll'),\n",
       " ('We', 'll', 'learn'),\n",
       " ('ll', 'learn', 'many'),\n",
       " ('learn', 'many', 'things'),\n",
       " ('many', 'things', 'in'),\n",
       " ('things', 'in', 'this'),\n",
       " ('in', 'this', 'no'),\n",
       " ('this', 'no', '1'),\n",
       " ('no', '1', 'lab'),\n",
       " ('1', 'lab', 'so'),\n",
       " ('lab', 'so', 'we'),\n",
       " ('so', 'we', 'will'),\n",
       " ('we', 'will', 'take'),\n",
       " ('will', 'take', 'it'),\n",
       " ('take', 'it', 'easy'),\n",
       " ('it', 'easy', 'Natural'),\n",
       " ('easy', 'Natural', 'Language'),\n",
       " ('Natural', 'Language', 'Processing'),\n",
       " ('Language', 'Processing', 'is'),\n",
       " ('Processing', 'is', 'fun')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-shooting",
   "metadata": {},
   "source": [
    "If we want to include the n-grams as strings rather than tuples, then we need to convert them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "twenty-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: ['Welcome to', 'to the', 'the Natural', 'Natural Language', 'Language Processing', 'Processing lab', 'lab We', 'We ll', 'll learn', 'learn many', 'many things', 'things in', 'in this', 'this no', 'no 1', '1 lab', 'lab so', 'so we', 'we will', 'will take', 'take it', 'it easy', 'easy Natural', 'Natural Language', 'Language Processing', 'Processing is', 'is fun']\n",
      "\n",
      "Trigrams: ['Welcome to the', 'to the Natural', 'the Natural Language', 'Natural Language Processing', 'Language Processing lab', 'Processing lab We', 'lab We ll', 'We ll learn', 'll learn many', 'learn many things', 'many things in', 'things in this', 'in this no', 'this no 1', 'no 1 lab', '1 lab so', 'lab so we', 'so we will', 'we will take', 'will take it', 'take it easy', 'it easy Natural', 'easy Natural Language', 'Natural Language Processing', 'Language Processing is', 'Processing is fun']\n"
     ]
    }
   ],
   "source": [
    "bigrams = [\" \".join(x) for x in list(ngrams(tokens, 2))]\n",
    "print(f\"Bigrams: {bigrams}\\n\")\n",
    "\n",
    "trigrams = [\" \".join(x) for x in list(ngrams(tokens, 3))]\n",
    "print(f\"Trigrams: {trigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-syracuse",
   "metadata": {},
   "source": [
    "Another important step we looked at in the lectures are the stop words. Let's try to use the nltk stop word list to remove them. \n",
    "\n",
    "These are words that are *commonly used* in a language but are typically *filtered out* during text processing because they are considered to carry little to no meaning on their own.\n",
    "\n",
    "First, let's download the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "apparent-going",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-disorder",
   "metadata": {},
   "source": [
    "and now check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pending-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(f\"number of stopwords: {len(stop_words)}\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-expression",
   "metadata": {},
   "source": [
    "Other libs have different stopwords. Let's see a much larger set from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "monthly-armenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 318\n",
      "frozenset({'without', 'mine', 'another', 'almost', 'further', 'thereupon', 'it', 'third', 'until', 'when', 'for', 'only', 'every', 'nevertheless', 'somehow', 'neither', 'formerly', 'find', 'if', 'all', 'get', 'mostly', 'while', 'thus', 'wherein', 'three', 'around', 'ourselves', 'should', 'except', 'have', 'seem', 'anyone', 'thick', 'take', 'con', 'again', 'fire', 'seeming', 'couldnt', 'a', 'on', 'whereafter', 'this', 'will', 'whereby', 'few', 'against', 'must', 'before', 'mill', 'etc', 'thence', 'our', 'interest', 'thru', 'within', 'done', 'herself', 'hereupon', 'yet', 'un', 'been', 'was', 'nothing', 'still', 'empty', 'anyway', 'two', 'anyhow', 'sometime', 'together', 'up', 'whenever', 'between', 'becoming', 'me', 'everywhere', 'themselves', 'namely', 'since', 'fill', 'more', 'beside', 'four', 'ie', 'us', 'wherever', 'several', 'of', 'itself', 'latterly', 'please', 'off', 'show', 'any', 'first', 'give', 'also', 'well', 'whereupon', 'part', 'might', 'down', 'nor', 'alone', 'hundred', 'due', 'twelve', 'seems', 'detail', 'some', 'move', 'by', 'you', 'became', 'behind', 'but', 'cannot', 'is', 'both', 'everyone', 'sixty', 'what', 'then', 'anywhere', 'she', 'besides', 'go', 'himself', 'thereafter', 'never', 'least', 'anything', 'meanwhile', 'ours', 'their', 'hence', 'bottom', 'with', 'thereby', 'throughout', 'although', 'toward', 'someone', 'we', 'rather', 'as', 'many', 'amount', 'afterwards', 'describe', 'side', 'however', 'most', 'latter', 'has', 'co', 'ten', 'had', 'serious', 'his', 'otherwise', 'among', 'whither', 'there', 'back', 'hereby', 'front', 'nine', 'from', 'beforehand', 'yourself', 'amoungst', 'her', 'where', 'somewhere', 'name', 'eleven', 'last', 'therefore', 'ltd', 'much', 'once', 'whole', 'own', 'de', 'along', 'do', 'ever', 'how', 'an', 'via', 'six', 'made', 'else', 'than', 'whereas', 'same', 'hers', 'and', 'whoever', 'yours', 'cry', 'five', 'eight', 'see', 'that', 'so', 'he', 'keep', 'above', 'elsewhere', 'therein', 'because', 'nobody', 'am', 'hereafter', 'onto', 'being', 'hasnt', 'myself', 'something', 'your', 'below', 'or', 'in', 'be', 'the', 'can', 'next', 'sincere', 'not', 'could', 'other', 'twenty', 'about', 'often', 'former', 'whence', 'system', 'no', 'even', 'over', 'are', 'whether', 'why', 'already', 'top', 'others', 'found', 'may', 'indeed', 'they', 'sometimes', 'its', 'inc', 'cant', 'yourselves', 'under', 'whose', 'always', 'less', 'bill', 'everything', 'would', 'noone', 'amongst', 'were', 'very', 'through', 'those', 'during', 'into', 'my', 'out', 'nowhere', 'put', 're', 'herein', 'i', 'at', 'to', 'seemed', 'thin', 'whatever', 'now', 'either', 'call', 'which', 'eg', 'full', 'forty', 'after', 'per', 'these', 'whom', 'though', 'him', 'towards', 'moreover', 'across', 'none', 'too', 'fifty', 'enough', 'each', 'who', 'perhaps', 'fifteen', 'becomes', 'one', 'upon', 'here', 'become', 'beyond', 'them', 'such'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "\n",
    "print(f\"number of stopwords: {len(sklearn_stop_words)}\")\n",
    "print(sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-deviation",
   "metadata": {},
   "source": [
    "Strangely enough, although there are more stop words in sklearn, you will find that nltk has words that are not contained in sklearn. So you might want to join the two lists.\n",
    "\n",
    "For normalising the text you could do something as simple as making sure all words are lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "alive-employee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'processing', 'lab', 'we', 'll', 'learn', 'many', 'things', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'natural', 'language', 'processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "norm_tokens = [x.lower() for x in tokens]\n",
    "print(norm_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-inflation",
   "metadata": {},
   "source": [
    "For stemming the words we could use nltk again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "upset-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcom', 'to', 'the', 'natur', 'languag', 'process', 'lab', 'we', 'll', 'learn', 'mani', 'thing', 'in', 'thi', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easi', 'natur', 'languag', 'process', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem_tokens = [stemmer.stem(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-porter",
   "metadata": {},
   "source": [
    "For lemmatising, nltk also does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "desperate-diploma",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'processing', 'lab', 'we', 'll', 'learn', 'many', 'thing', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'natural', 'language', 'processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stem_tokens = [lemmatizer.lemmatize(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-relevance",
   "metadata": {},
   "source": [
    "With this example sentence, there are no issues with the lemmatisation... but let's look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "understanding-veteran",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", \"a\"))  # declaring the POS as adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-martin",
   "metadata": {},
   "source": [
    "If we don't include the Part-of-speech (POS), nltk, using wordnet, does not work well. So let's try to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "russian-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map the POS tag to the first character lemmatize() accepts.\"\"\"\n",
    "\n",
    "    try:  # download nltk's POS tagger if it doesn't exist\n",
    "        nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()  # use ntlk's POS tagger on the word\n",
    "\n",
    "    # now we need to convert from nltk to wordnet POS notations (for compatibility reasons)\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # return and default to noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "imperial-comparative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'processing', 'lab', 'we', 'll', 'learn', 'many', 'thing', 'in', 'this', 'no', '1', 'lab', 'so', 'we', 'will', 'take', 'it', 'easy', 'natural', 'language', 'processing', 'be', 'fun']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stem_tokens = [lemmatizer.lemmatize(x, pos=get_wordnet_pos(x)) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-assignment",
   "metadata": {},
   "source": [
    "If we look at the words now we are getting more counts for our bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "polished-simon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'welcome': 1,\n",
       "         'to': 1,\n",
       "         'the': 1,\n",
       "         'natural': 2,\n",
       "         'language': 2,\n",
       "         'processing': 2,\n",
       "         'lab': 2,\n",
       "         'we': 2,\n",
       "         'll': 1,\n",
       "         'learn': 1,\n",
       "         'many': 1,\n",
       "         'thing': 1,\n",
       "         'in': 1,\n",
       "         'this': 1,\n",
       "         'no': 1,\n",
       "         '1': 1,\n",
       "         'so': 1,\n",
       "         'will': 1,\n",
       "         'take': 1,\n",
       "         'it': 1,\n",
       "         'easy': 1,\n",
       "         'be': 1,\n",
       "         'fun': 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bow = Counter(stem_tokens)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-healthcare",
   "metadata": {},
   "source": [
    "Now let's check the most frequent 6 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "unable-champagne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural', 2),\n",
       " ('language', 2),\n",
       " ('processing', 2),\n",
       " ('lab', 2),\n",
       " ('we', 2),\n",
       " ('welcome', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.most_common(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-hayes",
   "metadata": {},
   "source": [
    "Now let's remove the stop words and check the count again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "graduate-showcase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'welcome': 1,\n",
       "         'natural': 2,\n",
       "         'language': 2,\n",
       "         'processing': 2,\n",
       "         'lab': 2,\n",
       "         'learn': 1,\n",
       "         'many': 1,\n",
       "         'thing': 1,\n",
       "         '1': 1,\n",
       "         'take': 1,\n",
       "         'easy': 1,\n",
       "         'fun': 1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_tokens = [x for x in stem_tokens if x not in stop_words]\n",
    "count = Counter(no_stop_tokens)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-favor",
   "metadata": {},
   "source": [
    "Finally... let's make our feature vector using the frequency ratio (term count / total number of terms in the doc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "retained-medicine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125, 0.125, 0.125, 0.125, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]\n"
     ]
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(no_stop_tokens)\n",
    "for key, value in count.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "print(document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-spell",
   "metadata": {},
   "source": [
    "We have explored many many options already and we will continue with more advanced feature vectors in the next lab, plus some visualisations in charts. So until then please try different experiments on your own:\n",
    "1. See if you can change the text and have more sentences with different topics (so you can compare the feature vectors later)\n",
    "2. Try to use different libraries for tokenising, PoS tagging, stemming and lemmatising.\n",
    "3. Try to use distance metrics to compare vectors, such as Euclidian distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2459147",
   "metadata": {},
   "source": [
    "### Tokenization in Transformers architecture models like BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ea760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e4c61c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "sample = 'hello world'\n",
    "###\n",
    "\n",
    "import transformers \n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    return_attention_mask=False,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "tokenizer(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c204f7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7592, 2088, 102]\n",
      "['[CLS]', 'hello', 'world', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(sample)\n",
    "print(encoding)\n",
    "print(tokenizer.convert_ids_to_tokens(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba9a6b",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "The tokenizer’s vocabulary is the list of every token that it is capable of tokenizing. For now, you can think of a token as being analogous to a word. In the example, the input string consists of the tokens hello and world.\n",
    "\n",
    "To produce the IDs as shown in the example, the tokenizer looks up the index of each input token in the vocabulary. The vocabulary of a TFBertTokenizer object can be accessed using the vocab_list attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c820542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[unused0]': 1,\n",
       " '[unused1]': 2,\n",
       " '[unused2]': 3,\n",
       " '[unused3]': 4,\n",
       " '[unused4]': 5,\n",
       " '[unused5]': 6,\n",
       " '[unused6]': 7,\n",
       " '[unused7]': 8,\n",
       " '[unused8]': 9,\n",
       " '[unused9]': 10,\n",
       " '[unused10]': 11,\n",
       " '[unused11]': 12,\n",
       " '[unused12]': 13,\n",
       " '[unused13]': 14,\n",
       " '[unused14]': 15,\n",
       " '[unused15]': 16,\n",
       " '[unused16]': 17,\n",
       " '[unused17]': 18,\n",
       " '[unused18]': 19,\n",
       " '[unused19]': 20,\n",
       " '[unused20]': 21,\n",
       " '[unused21]': 22,\n",
       " '[unused22]': 23,\n",
       " '[unused23]': 24,\n",
       " '[unused24]': 25,\n",
       " '[unused25]': 26,\n",
       " '[unused26]': 27,\n",
       " '[unused27]': 28,\n",
       " '[unused28]': 29,\n",
       " '[unused29]': 30,\n",
       " '[unused30]': 31,\n",
       " '[unused31]': 32,\n",
       " '[unused32]': 33,\n",
       " '[unused33]': 34,\n",
       " '[unused34]': 35,\n",
       " '[unused35]': 36,\n",
       " '[unused36]': 37,\n",
       " '[unused37]': 38,\n",
       " '[unused38]': 39,\n",
       " '[unused39]': 40,\n",
       " '[unused40]': 41,\n",
       " '[unused41]': 42,\n",
       " '[unused42]': 43,\n",
       " '[unused43]': 44,\n",
       " '[unused44]': 45,\n",
       " '[unused45]': 46,\n",
       " '[unused46]': 47,\n",
       " '[unused47]': 48,\n",
       " '[unused48]': 49,\n",
       " '[unused49]': 50,\n",
       " '[unused50]': 51,\n",
       " '[unused51]': 52,\n",
       " '[unused52]': 53,\n",
       " '[unused53]': 54,\n",
       " '[unused54]': 55,\n",
       " '[unused55]': 56,\n",
       " '[unused56]': 57,\n",
       " '[unused57]': 58,\n",
       " '[unused58]': 59,\n",
       " '[unused59]': 60,\n",
       " '[unused60]': 61,\n",
       " '[unused61]': 62,\n",
       " '[unused62]': 63,\n",
       " '[unused63]': 64,\n",
       " '[unused64]': 65,\n",
       " '[unused65]': 66,\n",
       " '[unused66]': 67,\n",
       " '[unused67]': 68,\n",
       " '[unused68]': 69,\n",
       " '[unused69]': 70,\n",
       " '[unused70]': 71,\n",
       " '[unused71]': 72,\n",
       " '[unused72]': 73,\n",
       " '[unused73]': 74,\n",
       " '[unused74]': 75,\n",
       " '[unused75]': 76,\n",
       " '[unused76]': 77,\n",
       " '[unused77]': 78,\n",
       " '[unused78]': 79,\n",
       " '[unused79]': 80,\n",
       " '[unused80]': 81,\n",
       " '[unused81]': 82,\n",
       " '[unused82]': 83,\n",
       " '[unused83]': 84,\n",
       " '[unused84]': 85,\n",
       " '[unused85]': 86,\n",
       " '[unused86]': 87,\n",
       " '[unused87]': 88,\n",
       " '[unused88]': 89,\n",
       " '[unused89]': 90,\n",
       " '[unused90]': 91,\n",
       " '[unused91]': 92,\n",
       " '[unused92]': 93,\n",
       " '[unused93]': 94,\n",
       " '[unused94]': 95,\n",
       " '[unused95]': 96,\n",
       " '[unused96]': 97,\n",
       " '[unused97]': 98,\n",
       " '[unused98]': 99,\n",
       " '[UNK]': 100,\n",
       " '[CLS]': 101,\n",
       " '[SEP]': 102,\n",
       " '[MASK]': 103,\n",
       " '[unused99]': 104,\n",
       " '[unused100]': 105,\n",
       " '[unused101]': 106,\n",
       " '[unused102]': 107,\n",
       " '[unused103]': 108,\n",
       " '[unused104]': 109,\n",
       " '[unused105]': 110,\n",
       " '[unused106]': 111,\n",
       " '[unused107]': 112,\n",
       " '[unused108]': 113,\n",
       " '[unused109]': 114,\n",
       " '[unused110]': 115,\n",
       " '[unused111]': 116,\n",
       " '[unused112]': 117,\n",
       " '[unused113]': 118,\n",
       " '[unused114]': 119,\n",
       " '[unused115]': 120,\n",
       " '[unused116]': 121,\n",
       " '[unused117]': 122,\n",
       " '[unused118]': 123,\n",
       " '[unused119]': 124,\n",
       " '[unused120]': 125,\n",
       " '[unused121]': 126,\n",
       " '[unused122]': 127,\n",
       " '[unused123]': 128,\n",
       " '[unused124]': 129,\n",
       " '[unused125]': 130,\n",
       " '[unused126]': 131,\n",
       " '[unused127]': 132,\n",
       " '[unused128]': 133,\n",
       " '[unused129]': 134,\n",
       " '[unused130]': 135,\n",
       " '[unused131]': 136,\n",
       " '[unused132]': 137,\n",
       " '[unused133]': 138,\n",
       " '[unused134]': 139,\n",
       " '[unused135]': 140,\n",
       " '[unused136]': 141,\n",
       " '[unused137]': 142,\n",
       " '[unused138]': 143,\n",
       " '[unused139]': 144,\n",
       " '[unused140]': 145,\n",
       " '[unused141]': 146,\n",
       " '[unused142]': 147,\n",
       " '[unused143]': 148,\n",
       " '[unused144]': 149,\n",
       " '[unused145]': 150,\n",
       " '[unused146]': 151,\n",
       " '[unused147]': 152,\n",
       " '[unused148]': 153,\n",
       " '[unused149]': 154,\n",
       " '[unused150]': 155,\n",
       " '[unused151]': 156,\n",
       " '[unused152]': 157,\n",
       " '[unused153]': 158,\n",
       " '[unused154]': 159,\n",
       " '[unused155]': 160,\n",
       " '[unused156]': 161,\n",
       " '[unused157]': 162,\n",
       " '[unused158]': 163,\n",
       " '[unused159]': 164,\n",
       " '[unused160]': 165,\n",
       " '[unused161]': 166,\n",
       " '[unused162]': 167,\n",
       " '[unused163]': 168,\n",
       " '[unused164]': 169,\n",
       " '[unused165]': 170,\n",
       " '[unused166]': 171,\n",
       " '[unused167]': 172,\n",
       " '[unused168]': 173,\n",
       " '[unused169]': 174,\n",
       " '[unused170]': 175,\n",
       " '[unused171]': 176,\n",
       " '[unused172]': 177,\n",
       " '[unused173]': 178,\n",
       " '[unused174]': 179,\n",
       " '[unused175]': 180,\n",
       " '[unused176]': 181,\n",
       " '[unused177]': 182,\n",
       " '[unused178]': 183,\n",
       " '[unused179]': 184,\n",
       " '[unused180]': 185,\n",
       " '[unused181]': 186,\n",
       " '[unused182]': 187,\n",
       " '[unused183]': 188,\n",
       " '[unused184]': 189,\n",
       " '[unused185]': 190,\n",
       " '[unused186]': 191,\n",
       " '[unused187]': 192,\n",
       " '[unused188]': 193,\n",
       " '[unused189]': 194,\n",
       " '[unused190]': 195,\n",
       " '[unused191]': 196,\n",
       " '[unused192]': 197,\n",
       " '[unused193]': 198,\n",
       " '[unused194]': 199,\n",
       " '[unused195]': 200,\n",
       " '[unused196]': 201,\n",
       " '[unused197]': 202,\n",
       " '[unused198]': 203,\n",
       " '[unused199]': 204,\n",
       " '[unused200]': 205,\n",
       " '[unused201]': 206,\n",
       " '[unused202]': 207,\n",
       " '[unused203]': 208,\n",
       " '[unused204]': 209,\n",
       " '[unused205]': 210,\n",
       " '[unused206]': 211,\n",
       " '[unused207]': 212,\n",
       " '[unused208]': 213,\n",
       " '[unused209]': 214,\n",
       " '[unused210]': 215,\n",
       " '[unused211]': 216,\n",
       " '[unused212]': 217,\n",
       " '[unused213]': 218,\n",
       " '[unused214]': 219,\n",
       " '[unused215]': 220,\n",
       " '[unused216]': 221,\n",
       " '[unused217]': 222,\n",
       " '[unused218]': 223,\n",
       " '[unused219]': 224,\n",
       " '[unused220]': 225,\n",
       " '[unused221]': 226,\n",
       " '[unused222]': 227,\n",
       " '[unused223]': 228,\n",
       " '[unused224]': 229,\n",
       " '[unused225]': 230,\n",
       " '[unused226]': 231,\n",
       " '[unused227]': 232,\n",
       " '[unused228]': 233,\n",
       " '[unused229]': 234,\n",
       " '[unused230]': 235,\n",
       " '[unused231]': 236,\n",
       " '[unused232]': 237,\n",
       " '[unused233]': 238,\n",
       " '[unused234]': 239,\n",
       " '[unused235]': 240,\n",
       " '[unused236]': 241,\n",
       " '[unused237]': 242,\n",
       " '[unused238]': 243,\n",
       " '[unused239]': 244,\n",
       " '[unused240]': 245,\n",
       " '[unused241]': 246,\n",
       " '[unused242]': 247,\n",
       " '[unused243]': 248,\n",
       " '[unused244]': 249,\n",
       " '[unused245]': 250,\n",
       " '[unused246]': 251,\n",
       " '[unused247]': 252,\n",
       " '[unused248]': 253,\n",
       " '[unused249]': 254,\n",
       " '[unused250]': 255,\n",
       " '[unused251]': 256,\n",
       " '[unused252]': 257,\n",
       " '[unused253]': 258,\n",
       " '[unused254]': 259,\n",
       " '[unused255]': 260,\n",
       " '[unused256]': 261,\n",
       " '[unused257]': 262,\n",
       " '[unused258]': 263,\n",
       " '[unused259]': 264,\n",
       " '[unused260]': 265,\n",
       " '[unused261]': 266,\n",
       " '[unused262]': 267,\n",
       " '[unused263]': 268,\n",
       " '[unused264]': 269,\n",
       " '[unused265]': 270,\n",
       " '[unused266]': 271,\n",
       " '[unused267]': 272,\n",
       " '[unused268]': 273,\n",
       " '[unused269]': 274,\n",
       " '[unused270]': 275,\n",
       " '[unused271]': 276,\n",
       " '[unused272]': 277,\n",
       " '[unused273]': 278,\n",
       " '[unused274]': 279,\n",
       " '[unused275]': 280,\n",
       " '[unused276]': 281,\n",
       " '[unused277]': 282,\n",
       " '[unused278]': 283,\n",
       " '[unused279]': 284,\n",
       " '[unused280]': 285,\n",
       " '[unused281]': 286,\n",
       " '[unused282]': 287,\n",
       " '[unused283]': 288,\n",
       " '[unused284]': 289,\n",
       " '[unused285]': 290,\n",
       " '[unused286]': 291,\n",
       " '[unused287]': 292,\n",
       " '[unused288]': 293,\n",
       " '[unused289]': 294,\n",
       " '[unused290]': 295,\n",
       " '[unused291]': 296,\n",
       " '[unused292]': 297,\n",
       " '[unused293]': 298,\n",
       " '[unused294]': 299,\n",
       " '[unused295]': 300,\n",
       " '[unused296]': 301,\n",
       " '[unused297]': 302,\n",
       " '[unused298]': 303,\n",
       " '[unused299]': 304,\n",
       " '[unused300]': 305,\n",
       " '[unused301]': 306,\n",
       " '[unused302]': 307,\n",
       " '[unused303]': 308,\n",
       " '[unused304]': 309,\n",
       " '[unused305]': 310,\n",
       " '[unused306]': 311,\n",
       " '[unused307]': 312,\n",
       " '[unused308]': 313,\n",
       " '[unused309]': 314,\n",
       " '[unused310]': 315,\n",
       " '[unused311]': 316,\n",
       " '[unused312]': 317,\n",
       " '[unused313]': 318,\n",
       " '[unused314]': 319,\n",
       " '[unused315]': 320,\n",
       " '[unused316]': 321,\n",
       " '[unused317]': 322,\n",
       " '[unused318]': 323,\n",
       " '[unused319]': 324,\n",
       " '[unused320]': 325,\n",
       " '[unused321]': 326,\n",
       " '[unused322]': 327,\n",
       " '[unused323]': 328,\n",
       " '[unused324]': 329,\n",
       " '[unused325]': 330,\n",
       " '[unused326]': 331,\n",
       " '[unused327]': 332,\n",
       " '[unused328]': 333,\n",
       " '[unused329]': 334,\n",
       " '[unused330]': 335,\n",
       " '[unused331]': 336,\n",
       " '[unused332]': 337,\n",
       " '[unused333]': 338,\n",
       " '[unused334]': 339,\n",
       " '[unused335]': 340,\n",
       " '[unused336]': 341,\n",
       " '[unused337]': 342,\n",
       " '[unused338]': 343,\n",
       " '[unused339]': 344,\n",
       " '[unused340]': 345,\n",
       " '[unused341]': 346,\n",
       " '[unused342]': 347,\n",
       " '[unused343]': 348,\n",
       " '[unused344]': 349,\n",
       " '[unused345]': 350,\n",
       " '[unused346]': 351,\n",
       " '[unused347]': 352,\n",
       " '[unused348]': 353,\n",
       " '[unused349]': 354,\n",
       " '[unused350]': 355,\n",
       " '[unused351]': 356,\n",
       " '[unused352]': 357,\n",
       " '[unused353]': 358,\n",
       " '[unused354]': 359,\n",
       " '[unused355]': 360,\n",
       " '[unused356]': 361,\n",
       " '[unused357]': 362,\n",
       " '[unused358]': 363,\n",
       " '[unused359]': 364,\n",
       " '[unused360]': 365,\n",
       " '[unused361]': 366,\n",
       " '[unused362]': 367,\n",
       " '[unused363]': 368,\n",
       " '[unused364]': 369,\n",
       " '[unused365]': 370,\n",
       " '[unused366]': 371,\n",
       " '[unused367]': 372,\n",
       " '[unused368]': 373,\n",
       " '[unused369]': 374,\n",
       " '[unused370]': 375,\n",
       " '[unused371]': 376,\n",
       " '[unused372]': 377,\n",
       " '[unused373]': 378,\n",
       " '[unused374]': 379,\n",
       " '[unused375]': 380,\n",
       " '[unused376]': 381,\n",
       " '[unused377]': 382,\n",
       " '[unused378]': 383,\n",
       " '[unused379]': 384,\n",
       " '[unused380]': 385,\n",
       " '[unused381]': 386,\n",
       " '[unused382]': 387,\n",
       " '[unused383]': 388,\n",
       " '[unused384]': 389,\n",
       " '[unused385]': 390,\n",
       " '[unused386]': 391,\n",
       " '[unused387]': 392,\n",
       " '[unused388]': 393,\n",
       " '[unused389]': 394,\n",
       " '[unused390]': 395,\n",
       " '[unused391]': 396,\n",
       " '[unused392]': 397,\n",
       " '[unused393]': 398,\n",
       " '[unused394]': 399,\n",
       " '[unused395]': 400,\n",
       " '[unused396]': 401,\n",
       " '[unused397]': 402,\n",
       " '[unused398]': 403,\n",
       " '[unused399]': 404,\n",
       " '[unused400]': 405,\n",
       " '[unused401]': 406,\n",
       " '[unused402]': 407,\n",
       " '[unused403]': 408,\n",
       " '[unused404]': 409,\n",
       " '[unused405]': 410,\n",
       " '[unused406]': 411,\n",
       " '[unused407]': 412,\n",
       " '[unused408]': 413,\n",
       " '[unused409]': 414,\n",
       " '[unused410]': 415,\n",
       " '[unused411]': 416,\n",
       " '[unused412]': 417,\n",
       " '[unused413]': 418,\n",
       " '[unused414]': 419,\n",
       " '[unused415]': 420,\n",
       " '[unused416]': 421,\n",
       " '[unused417]': 422,\n",
       " '[unused418]': 423,\n",
       " '[unused419]': 424,\n",
       " '[unused420]': 425,\n",
       " '[unused421]': 426,\n",
       " '[unused422]': 427,\n",
       " '[unused423]': 428,\n",
       " '[unused424]': 429,\n",
       " '[unused425]': 430,\n",
       " '[unused426]': 431,\n",
       " '[unused427]': 432,\n",
       " '[unused428]': 433,\n",
       " '[unused429]': 434,\n",
       " '[unused430]': 435,\n",
       " '[unused431]': 436,\n",
       " '[unused432]': 437,\n",
       " '[unused433]': 438,\n",
       " '[unused434]': 439,\n",
       " '[unused435]': 440,\n",
       " '[unused436]': 441,\n",
       " '[unused437]': 442,\n",
       " '[unused438]': 443,\n",
       " '[unused439]': 444,\n",
       " '[unused440]': 445,\n",
       " '[unused441]': 446,\n",
       " '[unused442]': 447,\n",
       " '[unused443]': 448,\n",
       " '[unused444]': 449,\n",
       " '[unused445]': 450,\n",
       " '[unused446]': 451,\n",
       " '[unused447]': 452,\n",
       " '[unused448]': 453,\n",
       " '[unused449]': 454,\n",
       " '[unused450]': 455,\n",
       " '[unused451]': 456,\n",
       " '[unused452]': 457,\n",
       " '[unused453]': 458,\n",
       " '[unused454]': 459,\n",
       " '[unused455]': 460,\n",
       " '[unused456]': 461,\n",
       " '[unused457]': 462,\n",
       " '[unused458]': 463,\n",
       " '[unused459]': 464,\n",
       " '[unused460]': 465,\n",
       " '[unused461]': 466,\n",
       " '[unused462]': 467,\n",
       " '[unused463]': 468,\n",
       " '[unused464]': 469,\n",
       " '[unused465]': 470,\n",
       " '[unused466]': 471,\n",
       " '[unused467]': 472,\n",
       " '[unused468]': 473,\n",
       " '[unused469]': 474,\n",
       " '[unused470]': 475,\n",
       " '[unused471]': 476,\n",
       " '[unused472]': 477,\n",
       " '[unused473]': 478,\n",
       " '[unused474]': 479,\n",
       " '[unused475]': 480,\n",
       " '[unused476]': 481,\n",
       " '[unused477]': 482,\n",
       " '[unused478]': 483,\n",
       " '[unused479]': 484,\n",
       " '[unused480]': 485,\n",
       " '[unused481]': 486,\n",
       " '[unused482]': 487,\n",
       " '[unused483]': 488,\n",
       " '[unused484]': 489,\n",
       " '[unused485]': 490,\n",
       " '[unused486]': 491,\n",
       " '[unused487]': 492,\n",
       " '[unused488]': 493,\n",
       " '[unused489]': 494,\n",
       " '[unused490]': 495,\n",
       " '[unused491]': 496,\n",
       " '[unused492]': 497,\n",
       " '[unused493]': 498,\n",
       " '[unused494]': 499,\n",
       " '[unused495]': 500,\n",
       " '[unused496]': 501,\n",
       " '[unused497]': 502,\n",
       " '[unused498]': 503,\n",
       " '[unused499]': 504,\n",
       " '[unused500]': 505,\n",
       " '[unused501]': 506,\n",
       " '[unused502]': 507,\n",
       " '[unused503]': 508,\n",
       " '[unused504]': 509,\n",
       " '[unused505]': 510,\n",
       " '[unused506]': 511,\n",
       " '[unused507]': 512,\n",
       " '[unused508]': 513,\n",
       " '[unused509]': 514,\n",
       " '[unused510]': 515,\n",
       " '[unused511]': 516,\n",
       " '[unused512]': 517,\n",
       " '[unused513]': 518,\n",
       " '[unused514]': 519,\n",
       " '[unused515]': 520,\n",
       " '[unused516]': 521,\n",
       " '[unused517]': 522,\n",
       " '[unused518]': 523,\n",
       " '[unused519]': 524,\n",
       " '[unused520]': 525,\n",
       " '[unused521]': 526,\n",
       " '[unused522]': 527,\n",
       " '[unused523]': 528,\n",
       " '[unused524]': 529,\n",
       " '[unused525]': 530,\n",
       " '[unused526]': 531,\n",
       " '[unused527]': 532,\n",
       " '[unused528]': 533,\n",
       " '[unused529]': 534,\n",
       " '[unused530]': 535,\n",
       " '[unused531]': 536,\n",
       " '[unused532]': 537,\n",
       " '[unused533]': 538,\n",
       " '[unused534]': 539,\n",
       " '[unused535]': 540,\n",
       " '[unused536]': 541,\n",
       " '[unused537]': 542,\n",
       " '[unused538]': 543,\n",
       " '[unused539]': 544,\n",
       " '[unused540]': 545,\n",
       " '[unused541]': 546,\n",
       " '[unused542]': 547,\n",
       " '[unused543]': 548,\n",
       " '[unused544]': 549,\n",
       " '[unused545]': 550,\n",
       " '[unused546]': 551,\n",
       " '[unused547]': 552,\n",
       " '[unused548]': 553,\n",
       " '[unused549]': 554,\n",
       " '[unused550]': 555,\n",
       " '[unused551]': 556,\n",
       " '[unused552]': 557,\n",
       " '[unused553]': 558,\n",
       " '[unused554]': 559,\n",
       " '[unused555]': 560,\n",
       " '[unused556]': 561,\n",
       " '[unused557]': 562,\n",
       " '[unused558]': 563,\n",
       " '[unused559]': 564,\n",
       " '[unused560]': 565,\n",
       " '[unused561]': 566,\n",
       " '[unused562]': 567,\n",
       " '[unused563]': 568,\n",
       " '[unused564]': 569,\n",
       " '[unused565]': 570,\n",
       " '[unused566]': 571,\n",
       " '[unused567]': 572,\n",
       " '[unused568]': 573,\n",
       " '[unused569]': 574,\n",
       " '[unused570]': 575,\n",
       " '[unused571]': 576,\n",
       " '[unused572]': 577,\n",
       " '[unused573]': 578,\n",
       " '[unused574]': 579,\n",
       " '[unused575]': 580,\n",
       " '[unused576]': 581,\n",
       " '[unused577]': 582,\n",
       " '[unused578]': 583,\n",
       " '[unused579]': 584,\n",
       " '[unused580]': 585,\n",
       " '[unused581]': 586,\n",
       " '[unused582]': 587,\n",
       " '[unused583]': 588,\n",
       " '[unused584]': 589,\n",
       " '[unused585]': 590,\n",
       " '[unused586]': 591,\n",
       " '[unused587]': 592,\n",
       " '[unused588]': 593,\n",
       " '[unused589]': 594,\n",
       " '[unused590]': 595,\n",
       " '[unused591]': 596,\n",
       " '[unused592]': 597,\n",
       " '[unused593]': 598,\n",
       " '[unused594]': 599,\n",
       " '[unused595]': 600,\n",
       " '[unused596]': 601,\n",
       " '[unused597]': 602,\n",
       " '[unused598]': 603,\n",
       " '[unused599]': 604,\n",
       " '[unused600]': 605,\n",
       " '[unused601]': 606,\n",
       " '[unused602]': 607,\n",
       " '[unused603]': 608,\n",
       " '[unused604]': 609,\n",
       " '[unused605]': 610,\n",
       " '[unused606]': 611,\n",
       " '[unused607]': 612,\n",
       " '[unused608]': 613,\n",
       " '[unused609]': 614,\n",
       " '[unused610]': 615,\n",
       " '[unused611]': 616,\n",
       " '[unused612]': 617,\n",
       " '[unused613]': 618,\n",
       " '[unused614]': 619,\n",
       " '[unused615]': 620,\n",
       " '[unused616]': 621,\n",
       " '[unused617]': 622,\n",
       " '[unused618]': 623,\n",
       " '[unused619]': 624,\n",
       " '[unused620]': 625,\n",
       " '[unused621]': 626,\n",
       " '[unused622]': 627,\n",
       " '[unused623]': 628,\n",
       " '[unused624]': 629,\n",
       " '[unused625]': 630,\n",
       " '[unused626]': 631,\n",
       " '[unused627]': 632,\n",
       " '[unused628]': 633,\n",
       " '[unused629]': 634,\n",
       " '[unused630]': 635,\n",
       " '[unused631]': 636,\n",
       " '[unused632]': 637,\n",
       " '[unused633]': 638,\n",
       " '[unused634]': 639,\n",
       " '[unused635]': 640,\n",
       " '[unused636]': 641,\n",
       " '[unused637]': 642,\n",
       " '[unused638]': 643,\n",
       " '[unused639]': 644,\n",
       " '[unused640]': 645,\n",
       " '[unused641]': 646,\n",
       " '[unused642]': 647,\n",
       " '[unused643]': 648,\n",
       " '[unused644]': 649,\n",
       " '[unused645]': 650,\n",
       " '[unused646]': 651,\n",
       " '[unused647]': 652,\n",
       " '[unused648]': 653,\n",
       " '[unused649]': 654,\n",
       " '[unused650]': 655,\n",
       " '[unused651]': 656,\n",
       " '[unused652]': 657,\n",
       " '[unused653]': 658,\n",
       " '[unused654]': 659,\n",
       " '[unused655]': 660,\n",
       " '[unused656]': 661,\n",
       " '[unused657]': 662,\n",
       " '[unused658]': 663,\n",
       " '[unused659]': 664,\n",
       " '[unused660]': 665,\n",
       " '[unused661]': 666,\n",
       " '[unused662]': 667,\n",
       " '[unused663]': 668,\n",
       " '[unused664]': 669,\n",
       " '[unused665]': 670,\n",
       " '[unused666]': 671,\n",
       " '[unused667]': 672,\n",
       " '[unused668]': 673,\n",
       " '[unused669]': 674,\n",
       " '[unused670]': 675,\n",
       " '[unused671]': 676,\n",
       " '[unused672]': 677,\n",
       " '[unused673]': 678,\n",
       " '[unused674]': 679,\n",
       " '[unused675]': 680,\n",
       " '[unused676]': 681,\n",
       " '[unused677]': 682,\n",
       " '[unused678]': 683,\n",
       " '[unused679]': 684,\n",
       " '[unused680]': 685,\n",
       " '[unused681]': 686,\n",
       " '[unused682]': 687,\n",
       " '[unused683]': 688,\n",
       " '[unused684]': 689,\n",
       " '[unused685]': 690,\n",
       " '[unused686]': 691,\n",
       " '[unused687]': 692,\n",
       " '[unused688]': 693,\n",
       " '[unused689]': 694,\n",
       " '[unused690]': 695,\n",
       " '[unused691]': 696,\n",
       " '[unused692]': 697,\n",
       " '[unused693]': 698,\n",
       " '[unused694]': 699,\n",
       " '[unused695]': 700,\n",
       " '[unused696]': 701,\n",
       " '[unused697]': 702,\n",
       " '[unused698]': 703,\n",
       " '[unused699]': 704,\n",
       " '[unused700]': 705,\n",
       " '[unused701]': 706,\n",
       " '[unused702]': 707,\n",
       " '[unused703]': 708,\n",
       " '[unused704]': 709,\n",
       " '[unused705]': 710,\n",
       " '[unused706]': 711,\n",
       " '[unused707]': 712,\n",
       " '[unused708]': 713,\n",
       " '[unused709]': 714,\n",
       " '[unused710]': 715,\n",
       " '[unused711]': 716,\n",
       " '[unused712]': 717,\n",
       " '[unused713]': 718,\n",
       " '[unused714]': 719,\n",
       " '[unused715]': 720,\n",
       " '[unused716]': 721,\n",
       " '[unused717]': 722,\n",
       " '[unused718]': 723,\n",
       " '[unused719]': 724,\n",
       " '[unused720]': 725,\n",
       " '[unused721]': 726,\n",
       " '[unused722]': 727,\n",
       " '[unused723]': 728,\n",
       " '[unused724]': 729,\n",
       " '[unused725]': 730,\n",
       " '[unused726]': 731,\n",
       " '[unused727]': 732,\n",
       " '[unused728]': 733,\n",
       " '[unused729]': 734,\n",
       " '[unused730]': 735,\n",
       " '[unused731]': 736,\n",
       " '[unused732]': 737,\n",
       " '[unused733]': 738,\n",
       " '[unused734]': 739,\n",
       " '[unused735]': 740,\n",
       " '[unused736]': 741,\n",
       " '[unused737]': 742,\n",
       " '[unused738]': 743,\n",
       " '[unused739]': 744,\n",
       " '[unused740]': 745,\n",
       " '[unused741]': 746,\n",
       " '[unused742]': 747,\n",
       " '[unused743]': 748,\n",
       " '[unused744]': 749,\n",
       " '[unused745]': 750,\n",
       " '[unused746]': 751,\n",
       " '[unused747]': 752,\n",
       " '[unused748]': 753,\n",
       " '[unused749]': 754,\n",
       " '[unused750]': 755,\n",
       " '[unused751]': 756,\n",
       " '[unused752]': 757,\n",
       " '[unused753]': 758,\n",
       " '[unused754]': 759,\n",
       " '[unused755]': 760,\n",
       " '[unused756]': 761,\n",
       " '[unused757]': 762,\n",
       " '[unused758]': 763,\n",
       " '[unused759]': 764,\n",
       " '[unused760]': 765,\n",
       " '[unused761]': 766,\n",
       " '[unused762]': 767,\n",
       " '[unused763]': 768,\n",
       " '[unused764]': 769,\n",
       " '[unused765]': 770,\n",
       " '[unused766]': 771,\n",
       " '[unused767]': 772,\n",
       " '[unused768]': 773,\n",
       " '[unused769]': 774,\n",
       " '[unused770]': 775,\n",
       " '[unused771]': 776,\n",
       " '[unused772]': 777,\n",
       " '[unused773]': 778,\n",
       " '[unused774]': 779,\n",
       " '[unused775]': 780,\n",
       " '[unused776]': 781,\n",
       " '[unused777]': 782,\n",
       " '[unused778]': 783,\n",
       " '[unused779]': 784,\n",
       " '[unused780]': 785,\n",
       " '[unused781]': 786,\n",
       " '[unused782]': 787,\n",
       " '[unused783]': 788,\n",
       " '[unused784]': 789,\n",
       " '[unused785]': 790,\n",
       " '[unused786]': 791,\n",
       " '[unused787]': 792,\n",
       " '[unused788]': 793,\n",
       " '[unused789]': 794,\n",
       " '[unused790]': 795,\n",
       " '[unused791]': 796,\n",
       " '[unused792]': 797,\n",
       " '[unused793]': 798,\n",
       " '[unused794]': 799,\n",
       " '[unused795]': 800,\n",
       " '[unused796]': 801,\n",
       " '[unused797]': 802,\n",
       " '[unused798]': 803,\n",
       " '[unused799]': 804,\n",
       " '[unused800]': 805,\n",
       " '[unused801]': 806,\n",
       " '[unused802]': 807,\n",
       " '[unused803]': 808,\n",
       " '[unused804]': 809,\n",
       " '[unused805]': 810,\n",
       " '[unused806]': 811,\n",
       " '[unused807]': 812,\n",
       " '[unused808]': 813,\n",
       " '[unused809]': 814,\n",
       " '[unused810]': 815,\n",
       " '[unused811]': 816,\n",
       " '[unused812]': 817,\n",
       " '[unused813]': 818,\n",
       " '[unused814]': 819,\n",
       " '[unused815]': 820,\n",
       " '[unused816]': 821,\n",
       " '[unused817]': 822,\n",
       " '[unused818]': 823,\n",
       " '[unused819]': 824,\n",
       " '[unused820]': 825,\n",
       " '[unused821]': 826,\n",
       " '[unused822]': 827,\n",
       " '[unused823]': 828,\n",
       " '[unused824]': 829,\n",
       " '[unused825]': 830,\n",
       " '[unused826]': 831,\n",
       " '[unused827]': 832,\n",
       " '[unused828]': 833,\n",
       " '[unused829]': 834,\n",
       " '[unused830]': 835,\n",
       " '[unused831]': 836,\n",
       " '[unused832]': 837,\n",
       " '[unused833]': 838,\n",
       " '[unused834]': 839,\n",
       " '[unused835]': 840,\n",
       " '[unused836]': 841,\n",
       " '[unused837]': 842,\n",
       " '[unused838]': 843,\n",
       " '[unused839]': 844,\n",
       " '[unused840]': 845,\n",
       " '[unused841]': 846,\n",
       " '[unused842]': 847,\n",
       " '[unused843]': 848,\n",
       " '[unused844]': 849,\n",
       " '[unused845]': 850,\n",
       " '[unused846]': 851,\n",
       " '[unused847]': 852,\n",
       " '[unused848]': 853,\n",
       " '[unused849]': 854,\n",
       " '[unused850]': 855,\n",
       " '[unused851]': 856,\n",
       " '[unused852]': 857,\n",
       " '[unused853]': 858,\n",
       " '[unused854]': 859,\n",
       " '[unused855]': 860,\n",
       " '[unused856]': 861,\n",
       " '[unused857]': 862,\n",
       " '[unused858]': 863,\n",
       " '[unused859]': 864,\n",
       " '[unused860]': 865,\n",
       " '[unused861]': 866,\n",
       " '[unused862]': 867,\n",
       " '[unused863]': 868,\n",
       " '[unused864]': 869,\n",
       " '[unused865]': 870,\n",
       " '[unused866]': 871,\n",
       " '[unused867]': 872,\n",
       " '[unused868]': 873,\n",
       " '[unused869]': 874,\n",
       " '[unused870]': 875,\n",
       " '[unused871]': 876,\n",
       " '[unused872]': 877,\n",
       " '[unused873]': 878,\n",
       " '[unused874]': 879,\n",
       " '[unused875]': 880,\n",
       " '[unused876]': 881,\n",
       " '[unused877]': 882,\n",
       " '[unused878]': 883,\n",
       " '[unused879]': 884,\n",
       " '[unused880]': 885,\n",
       " '[unused881]': 886,\n",
       " '[unused882]': 887,\n",
       " '[unused883]': 888,\n",
       " '[unused884]': 889,\n",
       " '[unused885]': 890,\n",
       " '[unused886]': 891,\n",
       " '[unused887]': 892,\n",
       " '[unused888]': 893,\n",
       " '[unused889]': 894,\n",
       " '[unused890]': 895,\n",
       " '[unused891]': 896,\n",
       " '[unused892]': 897,\n",
       " '[unused893]': 898,\n",
       " '[unused894]': 899,\n",
       " '[unused895]': 900,\n",
       " '[unused896]': 901,\n",
       " '[unused897]': 902,\n",
       " '[unused898]': 903,\n",
       " '[unused899]': 904,\n",
       " '[unused900]': 905,\n",
       " '[unused901]': 906,\n",
       " '[unused902]': 907,\n",
       " '[unused903]': 908,\n",
       " '[unused904]': 909,\n",
       " '[unused905]': 910,\n",
       " '[unused906]': 911,\n",
       " '[unused907]': 912,\n",
       " '[unused908]': 913,\n",
       " '[unused909]': 914,\n",
       " '[unused910]': 915,\n",
       " '[unused911]': 916,\n",
       " '[unused912]': 917,\n",
       " '[unused913]': 918,\n",
       " '[unused914]': 919,\n",
       " '[unused915]': 920,\n",
       " '[unused916]': 921,\n",
       " '[unused917]': 922,\n",
       " '[unused918]': 923,\n",
       " '[unused919]': 924,\n",
       " '[unused920]': 925,\n",
       " '[unused921]': 926,\n",
       " '[unused922]': 927,\n",
       " '[unused923]': 928,\n",
       " '[unused924]': 929,\n",
       " '[unused925]': 930,\n",
       " '[unused926]': 931,\n",
       " '[unused927]': 932,\n",
       " '[unused928]': 933,\n",
       " '[unused929]': 934,\n",
       " '[unused930]': 935,\n",
       " '[unused931]': 936,\n",
       " '[unused932]': 937,\n",
       " '[unused933]': 938,\n",
       " '[unused934]': 939,\n",
       " '[unused935]': 940,\n",
       " '[unused936]': 941,\n",
       " '[unused937]': 942,\n",
       " '[unused938]': 943,\n",
       " '[unused939]': 944,\n",
       " '[unused940]': 945,\n",
       " '[unused941]': 946,\n",
       " '[unused942]': 947,\n",
       " '[unused943]': 948,\n",
       " '[unused944]': 949,\n",
       " '[unused945]': 950,\n",
       " '[unused946]': 951,\n",
       " '[unused947]': 952,\n",
       " '[unused948]': 953,\n",
       " '[unused949]': 954,\n",
       " '[unused950]': 955,\n",
       " '[unused951]': 956,\n",
       " '[unused952]': 957,\n",
       " '[unused953]': 958,\n",
       " '[unused954]': 959,\n",
       " '[unused955]': 960,\n",
       " '[unused956]': 961,\n",
       " '[unused957]': 962,\n",
       " '[unused958]': 963,\n",
       " '[unused959]': 964,\n",
       " '[unused960]': 965,\n",
       " '[unused961]': 966,\n",
       " '[unused962]': 967,\n",
       " '[unused963]': 968,\n",
       " '[unused964]': 969,\n",
       " '[unused965]': 970,\n",
       " '[unused966]': 971,\n",
       " '[unused967]': 972,\n",
       " '[unused968]': 973,\n",
       " '[unused969]': 974,\n",
       " '[unused970]': 975,\n",
       " '[unused971]': 976,\n",
       " '[unused972]': 977,\n",
       " '[unused973]': 978,\n",
       " '[unused974]': 979,\n",
       " '[unused975]': 980,\n",
       " '[unused976]': 981,\n",
       " '[unused977]': 982,\n",
       " '[unused978]': 983,\n",
       " '[unused979]': 984,\n",
       " '[unused980]': 985,\n",
       " '[unused981]': 986,\n",
       " '[unused982]': 987,\n",
       " '[unused983]': 988,\n",
       " '[unused984]': 989,\n",
       " '[unused985]': 990,\n",
       " '[unused986]': 991,\n",
       " '[unused987]': 992,\n",
       " '[unused988]': 993,\n",
       " '[unused989]': 994,\n",
       " '[unused990]': 995,\n",
       " '[unused991]': 996,\n",
       " '[unused992]': 997,\n",
       " '[unused993]': 998,\n",
       " '!': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb195c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mkey(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworld\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mvocab(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworld\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'key'"
     ]
    }
   ],
   "source": [
    "print(f\"Index of 'hello': {tokenizer.vocab('hello')}\")\n",
    "# print(f\"Index of 'world': {tokenizer.vocab('world')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fad1b580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7592, 2088, 102]\n",
      "['[CLS]', 'hello', 'world', '[SEP]']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0153ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
